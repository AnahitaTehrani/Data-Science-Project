{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Dataset Preparation\n",
    "\n",
    "The Dataset of Spotify Stock News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category    datetime                                           headline  \\\n",
      "0  company  1740759981  Palantir, Nvidia Nixed But Netflix Remains On ...   \n",
      "1  company  1740684439  Tracking Chase Coleman's Tiger Global Portfoli...   \n",
      "2  company  1740664816  Spotify Technology (SPOT) is Attracting Invest...   \n",
      "3  company  1740658999  YouTube Surpasses Competitors in Streaming and...   \n",
      "4  company  1740593693  Spotify CEO Wants EU To Penalize Apple For Def...   \n",
      "\n",
      "          id                                              image related  \\\n",
      "0  132937882  https://media.zenfs.com/en/ibd.com/fc3f416bcfc...    SPOT   \n",
      "1  132901707  https://static.seekingalpha.com/cdn/s3/uploads...    SPOT   \n",
      "2  132937883  https://media.zenfs.com/en/zacks.com/bdc2850a4...    SPOT   \n",
      "3  132937884  https://media.zenfs.com/en/us.finance.gurufocu...    SPOT   \n",
      "4  132937885  https://media.zenfs.com/en/Benzinga/20dc68a2c9...    SPOT   \n",
      "\n",
      "         source                                            summary  \\\n",
      "0         Yahoo  A volatile market has shaken off Nvidia and Pa...   \n",
      "1  SeekingAlpha  Tiger Global's 13F reveals a $26.46B portfolio...   \n",
      "2         Yahoo  Spotify (SPOT) has been one of the stocks most...   \n",
      "3         Yahoo  Nielsen ranks YouTube ahead of Netflix and Hul...   \n",
      "4         Yahoo  Spotify Technology’s (NYSE:SPOT) CEO, Daniel E...   \n",
      "\n",
      "                                                 url        date  \n",
      "0  https://finnhub.io/api/news?id=578a14cc0987e6c...  2025-02-28  \n",
      "1  https://finnhub.io/api/news?id=c7d354e4ade5e63...  2025-02-27  \n",
      "2  https://finnhub.io/api/news?id=cec976f64d52fff...  2025-02-27  \n",
      "3  https://finnhub.io/api/news?id=1fd8ee4da1b2b5e...  2025-02-27  \n",
      "4  https://finnhub.io/api/news?id=e4ed9eb39665a16...  2025-02-26  \n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "     open      high      low   close     volume  adj_high  adj_low  adj_close  \\\n",
      "0  584.25  609.9200  580.000  608.01  4531895.0  609.9200  580.000     608.01   \n",
      "1  611.00  613.0000  586.000  590.76  1191837.0  613.0000  586.000     590.76   \n",
      "2  595.62  608.5294  592.890  603.13  2629946.0  608.5294  592.890     603.13   \n",
      "3  597.22  599.1200  575.535  588.57  2822820.0  599.1200  575.535     588.57   \n",
      "4  612.30  621.9100  592.980  601.61  2078629.0  621.9100  592.980     601.61   \n",
      "\n",
      "   adj_open  adj_volume  split_factor  dividend symbol exchange        date  \n",
      "0    584.25   4531895.0           1.0       0.0   SPOT     XNYS  2025-02-28  \n",
      "1    611.00   1191837.0           1.0       0.0   SPOT     XNYS  2025-02-27  \n",
      "2    595.62   2629946.0           1.0       0.0   SPOT     XNYS  2025-02-26  \n",
      "3    597.22   2822820.0           1.0       0.0   SPOT     XNYS  2025-02-25  \n",
      "4    612.30   2078629.0           1.0       0.0   SPOT     XNYS  2025-02-24  \n"
     ]
    }
   ],
   "source": [
    "#load the csv file that contains the news from the finhub api 03/2024 to 02/2025\n",
    "df_news = pd.read_csv(\"/Users/armandocriscuolo/c2025/data_science_project_2025/code/Data-Science-Project/Data-Question-2-B/news_analysis/spotify_news_2024_2025_finhub.csv\")\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_news.head())\n",
    "\n",
    "print(\"\\n\\n-----------------------------------\\n\\n\")\n",
    "\n",
    "#load the csv file that contains the stock data from the marketstack api 03/2024 to 02/2025\n",
    "df_stock = pd.read_csv(\"/Users/armandocriscuolo/c2025/data_science_project_2025/code/Data-Science-Project/Data-Question-2-B/news_analysis/spotify_stock_data_20250323_193740.csv\")\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_stock.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spotify Stock Data\n",
      "         date    open      high      low   close     volume\n",
      "0  2025-02-28  584.25  609.9200  580.000  608.01  4531895.0\n",
      "1  2025-02-27  611.00  613.0000  586.000  590.76  1191837.0\n",
      "2  2025-02-26  595.62  608.5294  592.890  603.13  2629946.0\n",
      "3  2025-02-25  597.22  599.1200  575.535  588.57  2822820.0\n",
      "4  2025-02-24  612.30  621.9100  592.980  601.61  2078629.0\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "Spotify News Data\n",
      "         date                                           headline  \\\n",
      "0  2025-02-28  Palantir, Nvidia Nixed But Netflix Remains On ...   \n",
      "1  2025-02-27  Tracking Chase Coleman's Tiger Global Portfoli...   \n",
      "2  2025-02-27  Spotify Technology (SPOT) is Attracting Invest...   \n",
      "3  2025-02-27  YouTube Surpasses Competitors in Streaming and...   \n",
      "4  2025-02-26  Spotify CEO Wants EU To Penalize Apple For Def...   \n",
      "\n",
      "                                             summary  \n",
      "0  A volatile market has shaken off Nvidia and Pa...  \n",
      "1  Tiger Global's 13F reveals a $26.46B portfolio...  \n",
      "2  Spotify (SPOT) has been one of the stocks most...  \n",
      "3  Nielsen ranks YouTube ahead of Netflix and Hul...  \n",
      "4  Spotify Technology’s (NYSE:SPOT) CEO, Daniel E...  \n"
     ]
    }
   ],
   "source": [
    "#Lets clean the dataframe to have only the columns we need\n",
    "print(\"Spotify Stock Data\")\n",
    "df_stock = df_stock[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_stock.head())\n",
    "\n",
    "print(\"\\n\\n-----------------------------------\\n\\n\")\n",
    "\n",
    "print(\"Spotify News Data\")\n",
    "#lets clean the dataframe to have only the columns we need\n",
    "df_news = df_news[[\"date\", \"headline\", \"summary\"]]\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_news.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1444 entries, 0 to 1443\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            1444 non-null   datetime64[ns]\n",
      " 1   open            1444 non-null   float64       \n",
      " 2   high            1444 non-null   float64       \n",
      " 3   low             1444 non-null   float64       \n",
      " 4   close           1444 non-null   float64       \n",
      " 5   volume          1444 non-null   float64       \n",
      " 6   is_trading_day  1444 non-null   bool          \n",
      " 7   headline        1444 non-null   object        \n",
      " 8   summary         1444 non-null   object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 102.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert dates to datetime format\n",
    "df_stock['date'] = pd.to_datetime(df_stock['date'])\n",
    "df_news['date'] = pd.to_datetime(df_news['date'])\n",
    "\n",
    "# Create complete date range DataFrame\n",
    "start_date = min(df_stock['date'].min(), df_news['date'].min())\n",
    "end_date = max(df_stock['date'].max(), df_news['date'].max())\n",
    "date_range = pd.DataFrame({'date': pd.date_range(start=start_date, end=end_date)})\n",
    "\n",
    "# Merge stock data with complete date range and forward fill missing values\n",
    "complete_stock = pd.merge(date_range, df_stock, on='date', how='left')\n",
    "complete_stock = complete_stock.sort_values('date')\n",
    "complete_stock = complete_stock.ffill()  # Forward fill to use previous day's data for missing dates\n",
    "\n",
    "# Add indicator for trading days\n",
    "complete_stock['is_trading_day'] = complete_stock['date'].isin(df_stock['date'])\n",
    "\n",
    "# Now merge with news data, keeping one row per news item\n",
    "df_merged = pd.merge(complete_stock, df_news, on='date', how='outer')\n",
    "\n",
    "# Sort by date\n",
    "df_merged = df_merged.sort_values('date')\n",
    "\n",
    "# Fill NaN values in news columns\n",
    "df_merged['headline'] = df_merged['headline'].fillna('')\n",
    "df_merged['summary'] = df_merged['summary'].fillna('')\n",
    "\n",
    "print(df_merged.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicated headlines: 32\n",
      "\n",
      "Top 10 most duplicated headlines:\n",
      "headline\n",
      "                                                                                            73\n",
      "Spotify Technology (SPOT) is Attracting Investor Attention: Here is What You Should Know     5\n",
      "What You Missed On Wall Street This Morning                                                  4\n",
      "Spotify Technology (SPOT) Is a Trending Stock: Facts to Know Before Betting on It            4\n",
      "Is Trending Stock Spotify Technology (SPOT) a Buy Now?                                       3\n",
      "What You Missed On Wall Street On Tuesday                                                    3\n",
      "Morgan Stanley Reaffirms Their Buy Rating on Spotify Technology SA (SPOT)                    3\n",
      "Spotify Technology S.A. (SPOT): A Bull Case Theory                                           3\n",
      "A Closer Look at Spotify Technology's Options Market Dynamics                                3\n",
      "Spotify price target raised by $50 at KeyBanc, here's why                                    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Total number of duplicated summaries: 18\n",
      "\n",
      "Top 10 most duplicated summaries:\n",
      "summary\n",
      "Looking for stock market analysis and research with proves results? Zacks.com offers in-depth financial research with over 30years of proven results.                                                                                                                 432\n",
      "                                                                                                                                                                                                                                                                       92\n",
      "Recently, Zacks.com users have been paying close attention to Spotify (SPOT). This makes it worthwhile to examine what the stock has in store.                                                                                                                          7\n",
      "The average brokerage recommendation (ABR) for Spotify (SPOT) is equivalent to a Buy. The overly optimistic recommendations of Wall Street analysts make the effectiveness of this highly sought-after metric questionable. So, is it worth buying the stock?           6\n",
      "Spotify (SPOT) has been one of the stocks most watched by Zacks.com users lately. So, it is worth exploring what lies ahead for the stock.                                                                                                                              4\n",
      "Based on the average brokerage recommendation (ABR), Spotify (SPOT) should be added to one's portfolio. Wall Street analysts' overly optimistic recommendations cast doubt on the effectiveness of this highly sought-after metric. So, is the stock worth buying?      4\n",
      "Spotify (SPOT) shares have started gaining and might continue moving higher in the near term, as indicated by solid earnings estimate revisions.                                                                                                                        3\n",
      "Spotify (SPOT) doesn't possess the right combination of the two key ingredients for a likely earnings beat in its upcoming report. Get prepared with the key expectations.                                                                                              3\n",
      "Spotify (SPOT) has received quite a bit of attention from Zacks.com users lately. Therefore, it is wise to be aware of the facts that can impact the stock's prospects.                                                                                                 3\n",
      "Document UNITED STATES SECURITIES AND EXCHANGE...                                                                                                                                                                                                                       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated headline:\n",
      "Headline (appearing 73 times): \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated summary:\n",
      "Summary (appearing 432 times): Looking for stock market analysis and research with proves results? Zacks.com offers in-depth financial research with over 30years of proven results.\n"
     ]
    }
   ],
   "source": [
    "# Count duplicated headlines and summaries\n",
    "headline_counts = df_merged['headline'].value_counts()\n",
    "duplicated_headlines = headline_counts[headline_counts > 1]\n",
    "print(f\"Total number of duplicated headlines: {len(duplicated_headlines)}\")\n",
    "print(\"\\nTop 10 most duplicated headlines:\")\n",
    "print(duplicated_headlines.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "summary_counts = df_merged['summary'].value_counts()\n",
    "duplicated_summaries = summary_counts[summary_counts > 1]\n",
    "print(f\"Total number of duplicated summaries: {len(duplicated_summaries)}\")\n",
    "print(\"\\nTop 10 most duplicated summaries:\")\n",
    "print(duplicated_summaries.head(10))\n",
    "\n",
    "# Display the actual headlines/summaries with their dates\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated headline:\")\n",
    "if len(duplicated_headlines) > 0:\n",
    "    most_dup_headline = duplicated_headlines.index[0]\n",
    "    print(f\"Headline (appearing {duplicated_headlines.iloc[0]} times): {most_dup_headline}\")\n",
    "    #print(\"\\nDates when this headline appeared:\")\n",
    "    #for date in df_merged[df_merged['headline'] == most_dup_headline]['date']:\n",
    "    #    print(f\"- {date}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated summary:\")\n",
    "if len(duplicated_summaries) > 0:\n",
    "    most_dup_summary = duplicated_summaries.index[0]\n",
    "    print(f\"Summary (appearing {duplicated_summaries.iloc[0]} times): {most_dup_summary}\")\n",
    "    #print(\"\\nDates when this summary appeared:\")\n",
    "    #for date in df_merged[df_merged['summary'] == most_dup_summary]['date']:\n",
    "    #    print(f\"- {date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    open    high     low   close     volume  is_trading_day  \\\n",
      "0  2024-03-25  263.01  264.95  260.89  261.92   824685.0            True   \n",
      "11 2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "10 2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "8  2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "7  2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "\n",
      "                                             headline  \\\n",
      "0                                                       \n",
      "11  KeyBanc Keeps Their Buy Rating on Spotify Tech...   \n",
      "10  Taylor Swift Had A Monster Year, As Did The Mu...   \n",
      "8   Another List Throws Netflix (NASDAQ:NFLX) Out,...   \n",
      "7   Spotify initiated with bullish view at HSBC, h...   \n",
      "\n",
      "                                              summary  \n",
      "0                                                      \n",
      "11  Looking for stock market analysis and research...  \n",
      "10  Looking for stock market analysis and research...  \n",
      "8   Looking for stock market analysis and research...  \n",
      "7   Looking for stock market analysis and research...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1344 entries, 0 to 1443\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            1344 non-null   datetime64[ns]\n",
      " 1   open            1344 non-null   float64       \n",
      " 2   high            1344 non-null   float64       \n",
      " 3   low             1344 non-null   float64       \n",
      " 4   close           1344 non-null   float64       \n",
      " 5   volume          1344 non-null   float64       \n",
      " 6   is_trading_day  1344 non-null   bool          \n",
      " 7   headline        1344 non-null   object        \n",
      " 8   summary         1344 non-null   object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 95.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#lets delete the duplicates headlines and summaries from the dataframe\n",
    "df_merged = df_merged.drop_duplicates(subset=['headline', 'summary'])\n",
    "\n",
    "print(df_merged.head())\n",
    "print(df_merged.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe size: 1344 rows\n",
      "After removing specific summaries: 910 rows (removed 434 rows)\n",
      "After removing specific headlines: 910 rows (removed 0 rows)\n",
      "After removing duplicate headlines: 896 rows (removed 14 rows)\n",
      "After removing duplicate summaries: 879 rows (removed 17 rows)\n",
      "\n",
      "Total rows removed: 465\n",
      "Final dataframe size: 879 rows\n",
      "\n",
      "Remaining duplicated headlines: 0\n",
      "Remaining duplicated summaries: 0\n"
     ]
    }
   ],
   "source": [
    "# First, identify the specific summaries and headlines to completely remove\n",
    "summaries_to_remove = [\n",
    "    \"Looking for stock market analysis and research with proves results? Zacks.com offers in-depth financial research with over 30years of proven results.\",\n",
    "    \"\"  # Empty or whitespace summary\n",
    "]\n",
    "\n",
    "headlines_to_remove = [\"\"]  # Empty or whitespace headline\n",
    "\n",
    "# Get the original dataframe size\n",
    "original_size = len(df_merged)\n",
    "print(f\"Original dataframe size: {original_size} rows\")\n",
    "\n",
    "# Step 1: Completely remove rows with the specified summaries\n",
    "df_cleaned = df_merged[~df_merged['summary'].isin(summaries_to_remove)]\n",
    "step1_size = len(df_cleaned)\n",
    "print(f\"After removing specific summaries: {step1_size} rows (removed {original_size - step1_size} rows)\")\n",
    "\n",
    "# Step 2: Completely remove rows with the specified headlines\n",
    "df_cleaned = df_cleaned[~df_cleaned['headline'].isin(headlines_to_remove)]\n",
    "step2_size = len(df_cleaned)\n",
    "print(f\"After removing specific headlines: {step2_size} rows (removed {step1_size - step2_size} rows)\")\n",
    "\n",
    "# Step 3: For other duplicated headlines, keep only the first occurrence\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['headline'], keep='first')\n",
    "step3_size = len(df_cleaned)\n",
    "print(f\"After removing duplicate headlines: {step3_size} rows (removed {step2_size - step3_size} rows)\")\n",
    "\n",
    "# Step 4: For other duplicated summaries, keep only the first occurrence\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['summary'], keep='first')\n",
    "final_size = len(df_cleaned)\n",
    "print(f\"After removing duplicate summaries: {final_size} rows (removed {step3_size - final_size} rows)\")\n",
    "\n",
    "print(f\"\\nTotal rows removed: {original_size - final_size}\")\n",
    "print(f\"Final dataframe size: {final_size} rows\")\n",
    "\n",
    "# Check if there are any remaining duplicates\n",
    "remaining_dup_headlines = df_cleaned['headline'].duplicated().sum()\n",
    "remaining_dup_summaries = df_cleaned['summary'].duplicated().sum()\n",
    "print(f\"\\nRemaining duplicated headlines: {remaining_dup_headlines}\")\n",
    "print(f\"Remaining duplicated summaries: {remaining_dup_summaries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicated headlines: 0\n",
      "\n",
      "Top 10 most duplicated headlines:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Total number of duplicated summaries: 0\n",
      "\n",
      "Top 10 most duplicated summaries:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated headline:\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated summary:\n"
     ]
    }
   ],
   "source": [
    "#Lets check if there are still any duplicates in the dataframe\n",
    "\n",
    "# Count duplicated headlines and summaries\n",
    "headline_counts = df_cleaned['headline'].value_counts()\n",
    "duplicated_headlines = headline_counts[headline_counts > 1]\n",
    "print(f\"Total number of duplicated headlines: {len(duplicated_headlines)}\")\n",
    "print(\"\\nTop 10 most duplicated headlines:\")\n",
    "print(duplicated_headlines.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "summary_counts = df_cleaned['summary'].value_counts()\n",
    "duplicated_summaries = summary_counts[summary_counts > 1]\n",
    "print(f\"Total number of duplicated summaries: {len(duplicated_summaries)}\")\n",
    "print(\"\\nTop 10 most duplicated summaries:\")\n",
    "print(duplicated_summaries.head(10))\n",
    "\n",
    "# Display the actual headlines/summaries with their dates\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated headline:\")\n",
    "if len(duplicated_headlines) > 0:\n",
    "    most_dup_headline = duplicated_headlines.index[0]\n",
    "    print(f\"Headline (appearing {duplicated_headlines.iloc[0]} times): {most_dup_headline}\")\n",
    "    print(\"\\nDates when this headline appeared:\")\n",
    "    for date in df_cleaned[df_cleaned['headline'] == most_dup_headline]['date']:\n",
    "        print(f\"- {date}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated summary:\")\n",
    "if len(duplicated_summaries) > 0:\n",
    "    most_dup_summary = duplicated_summaries.index[0]\n",
    "    print(f\"Summary (appearing {duplicated_summaries.iloc[0]} times): {most_dup_summary}\")\n",
    "    print(\"\\nDates when this summary appeared:\")\n",
    "    for date in df_cleaned[df_cleaned['summary'] == most_dup_summary]['date']:\n",
    "        print(f\"- {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 879 entries, 3 to 1443\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            879 non-null    datetime64[ns]\n",
      " 1   open            879 non-null    float64       \n",
      " 2   high            879 non-null    float64       \n",
      " 3   low             879 non-null    float64       \n",
      " 4   close           879 non-null    float64       \n",
      " 5   volume          879 non-null    float64       \n",
      " 6   is_trading_day  879 non-null    bool          \n",
      " 7   headline        879 non-null    object        \n",
      " 8   summary         879 non-null    object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 62.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned Data\n",
    "\n",
    "Finally we have a cleaned dataset of all the news about Spotify and also stock data. \n",
    "\n",
    "### Two things to keep in mind:\n",
    "\n",
    "1. We used forward fill for the stock data to populate all dates that were not trading days.\n",
    "2. We have duplicate dates because there are days with many news articles and then again days without any news!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged dataframe\n",
    "df_cleaned.to_csv(\"/Users/armandocriscuolo/c2025/data_science_project_2025/code/Data-Science-Project/Data-Question-2-B/news_analysis/spotify_news_stock_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/armandocriscuolo/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Download NLTK resources (run this once)\n",
    "try:\n",
    "    nltk.data.find('vader_lexicon')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the Spotify stock and news data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Raw dataframe with stock and news data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed dataframe with additional financial metrics\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure date column is datetime if needed\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Calculate daily returns (percentage change in closing price)\n",
    "    df['daily_return'] = df['close'].pct_change() * 100\n",
    "    \n",
    "    # Calculate volatility (20-day rolling standard deviation of returns)\n",
    "    df['volatility_20d'] = df['daily_return'].rolling(window=20).std()\n",
    "    \n",
    "    # Calculate abnormal returns (daily return - average market return)\n",
    "    # Note: In a real analysis, you would use market index returns\n",
    "    df['abnormal_return'] = df['daily_return'] - df['daily_return'].mean()\n",
    "    \n",
    "    # Calculate volume change\n",
    "    df['volume_change'] = df['volume'].pct_change() * 100\n",
    "    \n",
    "    # Log transform volume (often helps with analysis)\n",
    "    df['log_volume'] = np.log(df['volume'].replace(0, 1))\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sentiment(df):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to news headlines and summaries\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with headline and summary columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Dataframe with added sentiment scores\n",
    "    \"\"\"\n",
    "    # Initialize VADER sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Fill NaN values with empty strings\n",
    "    df['headline'] = df['headline'].fillna('')\n",
    "    df['summary'] = df['summary'].fillna('')\n",
    "    \n",
    "    # Define a function to get sentiment scores\n",
    "    def get_sentiment(text):\n",
    "        if pd.isna(text) or text == '':\n",
    "            return 0\n",
    "        return sia.polarity_scores(str(text))['compound']\n",
    "    \n",
    "    # Apply sentiment analysis\n",
    "    print(\"Applying sentiment analysis to headlines and summaries...\")\n",
    "    df['headline_sentiment'] = df['headline'].apply(get_sentiment)\n",
    "    df['summary_sentiment'] = df['summary'].apply(get_sentiment)\n",
    "    \n",
    "    # Calculate weighted sentiment (headline has more impact)\n",
    "    df['combined_sentiment'] = 0.7 * df['headline_sentiment'] + 0.3 * df['summary_sentiment']\n",
    "    \n",
    "    # Create sentiment categories for analysis\n",
    "    df['sentiment_category'] = pd.cut(\n",
    "        df['combined_sentiment'],\n",
    "        bins=[-1.1, -0.2, 0.2, 1.1],\n",
    "        labels=['negative', 'neutral', 'positive']\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_daily_data(df):\n",
    "    \"\"\"\n",
    "    Aggregate multiple news items per day into single daily records\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with sentiment scores\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Dataframe with one row per day\n",
    "    \"\"\"\n",
    "    print(\"Aggregating data to daily level...\")\n",
    "    \n",
    "    # For news sentiment, we want the average per day\n",
    "    agg_functions = {\n",
    "        'headline_sentiment': 'mean',\n",
    "        'summary_sentiment': 'mean',\n",
    "        'combined_sentiment': 'mean',\n",
    "        # For stock data, we just take the first value (should be same for all rows on same day)\n",
    "        'open': 'first',\n",
    "        'high': 'first',\n",
    "        'low': 'first',\n",
    "        'close': 'first',\n",
    "        'volume': 'first',\n",
    "        'daily_return': 'first',\n",
    "        'volatility_20d': 'first',\n",
    "        'volume_change': 'first',\n",
    "        'abnormal_return': 'first',\n",
    "        'log_volume': 'first',\n",
    "        'is_trading_day': 'first'\n",
    "    }\n",
    "    \n",
    "    # Group by date and apply aggregation\n",
    "    daily_data = df.groupby(df.index).agg(agg_functions)\n",
    "    \n",
    "    # Count number of news articles per day\n",
    "    news_count = df.groupby(df.index).size()\n",
    "    daily_data['news_count'] = news_count\n",
    "    \n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_lagged_features(df, max_lag=5):\n",
    "    \"\"\"\n",
    "    Create lagged features for time series analysis\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Daily dataframe\n",
    "    max_lag (int): Maximum number of days to lag\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Dataframe with lagged features\n",
    "    \"\"\"\n",
    "    print(f\"Creating lagged features up to {max_lag} days...\")\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    # Create lagged sentiment features\n",
    "    for lag in range(1, max_lag+1):\n",
    "        df_lagged[f'sentiment_lag_{lag}'] = df_lagged['combined_sentiment'].shift(lag)\n",
    "    \n",
    "    # Create forward return (next day's return)\n",
    "    df_lagged['next_day_return'] = df_lagged['daily_return'].shift(-1)\n",
    "    \n",
    "    return df_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlation(df):\n",
    "    \"\"\"\n",
    "    Analyze correlation between sentiment and stock metrics\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Processed dataframe\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Correlation matrix and p-values matrix\n",
    "    \"\"\"\n",
    "    print(\"Analyzing correlations between sentiment and stock metrics...\")\n",
    "    \n",
    "    # Select relevant columns for correlation analysis\n",
    "    cols_for_corr = [\n",
    "        'headline_sentiment', 'summary_sentiment', 'combined_sentiment',\n",
    "        'daily_return', 'next_day_return', 'volatility_20d', 'volume_change', 'news_count'\n",
    "    ]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[cols_for_corr].corr()\n",
    "    \n",
    "    # Calculate statistical significance\n",
    "    p_values = pd.DataFrame(index=corr_matrix.index, columns=corr_matrix.columns)\n",
    "    \n",
    "    for i in corr_matrix.index:\n",
    "        for j in corr_matrix.columns:\n",
    "            if i != j:  # Skip diagonal\n",
    "                # Create a temporary dataframe with just the two columns we're looking at\n",
    "                # and drop rows where either column has a NaN value\n",
    "                temp_df = df[[i, j]].dropna()\n",
    "                if len(temp_df) >= 2:  # Need at least 2 points for correlation\n",
    "                    corr, p = stats.pearsonr(temp_df[i], temp_df[j])\n",
    "                    p_values.loc[i, j] = p\n",
    "                else:\n",
    "                    p_values.loc[i, j] = np.nan\n",
    "    \n",
    "    return corr_matrix, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_lagged_effects(df, max_lag=5):\n",
    "    \"\"\"\n",
    "    Analyze the effect of lagged sentiment on stock returns\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with lagged features\n",
    "    max_lag (int): Maximum lag to analyze\n",
    "    \n",
    "    Returns:\n",
    "    pandas.Series: Correlations of lagged sentiment with returns\n",
    "    \"\"\"\n",
    "    print(\"Analyzing lagged effects of sentiment on returns...\")\n",
    "    \n",
    "    # Get correlation of each lagged sentiment with returns\n",
    "    lagged_columns = [f'sentiment_lag_{i}' for i in range(1, max_lag+1)]\n",
    "    lagged_corr = df[['daily_return'] + lagged_columns].corr().loc['daily_return', lagged_columns]\n",
    "    \n",
    "    # Get correlation with next day's return\n",
    "    next_day_corr = df[['combined_sentiment', 'next_day_return']].corr().loc['combined_sentiment', 'next_day_return']\n",
    "    print(f\"Correlation with next day's return: {next_day_corr:.4f}\")\n",
    "    \n",
    "    return lagged_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_prediction_model(df, features=None):\n",
    "    \"\"\"\n",
    "    Build a machine learning model to predict returns based on sentiment\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with sentiment and return data\n",
    "    features (list): List of feature columns to use\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Feature importances, MSE scores, and R² scores\n",
    "    \"\"\"\n",
    "    print(\"Building prediction model for stock returns...\")\n",
    "    \n",
    "    if features is None:\n",
    "        features = [\n",
    "            'combined_sentiment', 'news_count', 'volatility_20d',\n",
    "            'volume_change', 'sentiment_lag_1', 'sentiment_lag_2'\n",
    "        ]\n",
    "    \n",
    "    # Filter rows with complete data\n",
    "    model_data = df.dropna(subset=features + ['next_day_return'])\n",
    "    model_data = model_data[model_data['is_trading_day']]  # Only use trading days\n",
    "    \n",
    "    X = model_data[features]\n",
    "    y = model_data['next_day_return']\n",
    "    \n",
    "    # Use time series split for validation (important for time series data)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "    r2_scores = []\n",
    "    feature_importances = None\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train random forest model\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = rf.predict(X_test)\n",
    "        mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        \n",
    "        # Store feature importance\n",
    "        if feature_importances is None:\n",
    "            feature_importances = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': rf.feature_importances_\n",
    "            })\n",
    "        else:\n",
    "            feature_importances['importance'] = (feature_importances['importance'] + rf.feature_importances_) / 2\n",
    "    \n",
    "    print(f\"Model evaluation - Mean MSE: {np.mean(mse_scores):.4f}, Mean R²: {np.mean(r2_scores):.4f}\")\n",
    "    return feature_importances.sort_values('importance', ascending=False), mse_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_results(df, daily_data, corr_matrix, lagged_corr, feature_importances):\n",
    "    \"\"\"\n",
    "    Create visualizations for the sentiment-stock analysis\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Original dataframe with sentiment\n",
    "    daily_data (pandas.DataFrame): Aggregated daily data\n",
    "    corr_matrix (pandas.DataFrame): Correlation matrix\n",
    "    lagged_corr (pandas.Series): Lagged correlation results\n",
    "    feature_importances (pandas.DataFrame): Feature importances from model\n",
    "    \"\"\"\n",
    "    print(\"Creating visualizations...\")\n",
    "    \n",
    "    # Figure 1: Stock price and sentiment over time\n",
    "    plt.figure(figsize=(12, 20))\n",
    "    \n",
    "    # Plot 1: Stock price and sentiment\n",
    "    plt.subplot(4, 1, 1)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(daily_data.index, daily_data['close'], 'b-', label='Close Price')\n",
    "    ax1.set_ylabel('Stock Price ($)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(daily_data.index, daily_data['combined_sentiment'], 'r-', alpha=0.7, label='Sentiment')\n",
    "    ax2.set_ylabel('Sentiment Score', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    plt.title('Spotify Stock Price and News Sentiment Over Time')\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    # Plot 2: Scatter plot of sentiment vs. daily return\n",
    "    plt.subplot(4, 1, 2)\n",
    "    trading_days = daily_data[daily_data['is_trading_day']]\n",
    "    sns.scatterplot(\n",
    "        x='combined_sentiment', \n",
    "        y='daily_return', \n",
    "        data=trading_days, \n",
    "        hue='news_count',\n",
    "        palette='viridis',\n",
    "        size='volume_change',\n",
    "        sizes=(20, 200),\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Add regression line\n",
    "    x = trading_days['combined_sentiment']\n",
    "    y = trading_days['daily_return']\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(x, p(x), \"r--\", alpha=0.7)\n",
    "    \n",
    "    plt.title('News Sentiment vs. Daily Return')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Daily Return (%)')\n",
    "    \n",
    "    # Plot 3: Correlation heatmap\n",
    "    plt.subplot(4, 1, 3)\n",
    "    sns.heatmap(\n",
    "        corr_matrix[['daily_return', 'next_day_return', 'volatility_20d', 'volume_change']]\n",
    "        .loc[['headline_sentiment', 'summary_sentiment', 'combined_sentiment', 'news_count']],\n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        vmin=-0.5, \n",
    "        vmax=0.5\n",
    "    )\n",
    "    plt.title('Correlation Between Sentiment and Stock Metrics')\n",
    "    \n",
    "    # Plot 4: Lagged effects analysis\n",
    "    plt.subplot(4, 1, 4)\n",
    "    lagged_corr.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Effect of Lagged Sentiment on Daily Returns')\n",
    "    plt.xlabel('Lag (Days)')\n",
    "    plt.ylabel('Correlation with Returns')\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('spotify_sentiment_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # Figure 2: Additional insights\n",
    "    plt.figure(figsize=(12, 15))\n",
    "    \n",
    "    # Plot 1: Returns by sentiment category\n",
    "    plt.subplot(3, 1, 1)\n",
    "    \n",
    "    # Recalculate sentiment categories for the daily data\n",
    "    trading_days['sentiment_category'] = pd.cut(\n",
    "        trading_days['combined_sentiment'],\n",
    "        bins=[-1.1, -0.2, 0.2, 1.1],\n",
    "        labels=['negative', 'neutral', 'positive']\n",
    "    )\n",
    "    \n",
    "    sns.boxplot(x='sentiment_category', y='daily_return', data=trading_days)\n",
    "    plt.title('Daily Returns by Sentiment Category')\n",
    "    plt.xlabel('Sentiment Category')\n",
    "    plt.ylabel('Daily Return (%)')\n",
    "    \n",
    "    # Perform ANOVA to check if differences are significant\n",
    "    sentiment_groups = [\n",
    "        trading_days[trading_days['sentiment_category'] == 'negative']['daily_return'].dropna(),\n",
    "        trading_days[trading_days['sentiment_category'] == 'neutral']['daily_return'].dropna(),\n",
    "        trading_days[trading_days['sentiment_category'] == 'positive']['daily_return'].dropna()\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        f_stat, p_val = stats.f_oneway(*sentiment_groups)\n",
    "        plt.annotate(f\"ANOVA: F={f_stat:.2f}, p={p_val:.4f}\", \n",
    "                    xy=(0.5, 0.9), xycoords='axes fraction', \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Plot 2: Feature importance from prediction model\n",
    "    plt.subplot(3, 1, 2)\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importances, palette='viridis')\n",
    "    plt.title('Feature Importance for Return Prediction')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "    # Plot 3: News count distribution and impact\n",
    "    plt.subplot(3, 1, 3)\n",
    "    sns.scatterplot(x='news_count', y='volatility_20d', \n",
    "                   size='combined_sentiment', hue='daily_return',\n",
    "                   data=trading_days, palette='RdYlGn', sizes=(20, 200))\n",
    "    plt.title('News Volume vs. Volatility')\n",
    "    plt.xlabel('Number of News Articles')\n",
    "    plt.ylabel('20-Day Volatility')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('spotify_sentiment_additional_insights.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spotify stock and news sentiment analysis...\n",
      "Preprocessed data shape: (879, 13)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 879 entries, 2024-03-27 to 2025-02-28\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   open             879 non-null    float64\n",
      " 1   high             879 non-null    float64\n",
      " 2   low              879 non-null    float64\n",
      " 3   close            879 non-null    float64\n",
      " 4   volume           879 non-null    float64\n",
      " 5   is_trading_day   879 non-null    bool   \n",
      " 6   headline         879 non-null    object \n",
      " 7   summary          879 non-null    object \n",
      " 8   daily_return     878 non-null    float64\n",
      " 9   volatility_20d   859 non-null    float64\n",
      " 10  abnormal_return  878 non-null    float64\n",
      " 11  volume_change    878 non-null    float64\n",
      " 12  log_volume       879 non-null    float64\n",
      "dtypes: bool(1), float64(10), object(2)\n",
      "memory usage: 90.1+ KB\n",
      "None\n",
      "              open     high     low  close     volume  is_trading_day  \\\n",
      "date                                                                    \n",
      "2024-03-27  267.00  269.720  257.56  260.2  1427218.0            True   \n",
      "2024-03-27  267.00  269.720  257.56  260.2  1427218.0            True   \n",
      "2024-03-27  267.00  269.720  257.56  260.2  1427218.0            True   \n",
      "2024-03-28  258.53  268.545  258.00  263.9  1371668.0            True   \n",
      "2024-03-28  258.53  268.545  258.00  263.9  1371668.0            True   \n",
      "\n",
      "                                                     headline  \\\n",
      "date                                                            \n",
      "2024-03-27  Wall Street Lunch: Trump Media Could Double Wi...   \n",
      "2024-03-27  Spotify initiated, Lowe's downgraded: Wall Str...   \n",
      "2024-03-27  After betting $1 billion on podcasts, Spotify ...   \n",
      "2024-03-28                    Bull of the Day: Spotify (SPOT)   \n",
      "2024-03-28  Spotify to Add New Features for Universal Musi...   \n",
      "\n",
      "                                                      summary  daily_return  \\\n",
      "date                                                                          \n",
      "2024-03-27  Trump Media's shares are rallying, sparking di...           NaN   \n",
      "2024-03-27  Spotify initiated, Lowe's downgraded: Wall Str...      0.000000   \n",
      "2024-03-27  Users will fork out $117 for an introductory p...      0.000000   \n",
      "2024-03-28  Spotify's embrace of new media has propelled i...      1.421983   \n",
      "2024-03-28  By Mauro Orru Spotify Technology will introduc...      0.000000   \n",
      "\n",
      "            volatility_20d  abnormal_return  volume_change  log_volume  \n",
      "date                                                                    \n",
      "2024-03-27             NaN              NaN            NaN   14.171238  \n",
      "2024-03-27             NaN        -0.106240       0.000000   14.171238  \n",
      "2024-03-27             NaN        -0.106240       0.000000   14.171238  \n",
      "2024-03-28             NaN         1.315743      -3.892187   14.131538  \n",
      "2024-03-28             NaN        -0.106240       0.000000   14.131538  \n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Applying sentiment analysis to headlines and summaries...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 879 entries, 2024-03-27 to 2025-02-28\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   open                879 non-null    float64 \n",
      " 1   high                879 non-null    float64 \n",
      " 2   low                 879 non-null    float64 \n",
      " 3   close               879 non-null    float64 \n",
      " 4   volume              879 non-null    float64 \n",
      " 5   is_trading_day      879 non-null    bool    \n",
      " 6   headline            879 non-null    object  \n",
      " 7   summary             879 non-null    object  \n",
      " 8   daily_return        878 non-null    float64 \n",
      " 9   volatility_20d      859 non-null    float64 \n",
      " 10  abnormal_return     878 non-null    float64 \n",
      " 11  volume_change       878 non-null    float64 \n",
      " 12  log_volume          879 non-null    float64 \n",
      " 13  headline_sentiment  879 non-null    float64 \n",
      " 14  summary_sentiment   879 non-null    float64 \n",
      " 15  combined_sentiment  879 non-null    float64 \n",
      " 16  sentiment_category  879 non-null    category\n",
      "dtypes: bool(1), category(1), float64(13), object(2)\n",
      "memory usage: 111.7+ KB\n",
      "None\n",
      "              open     high     low  close     volume  is_trading_day  \\\n",
      "date                                                                    \n",
      "2024-03-27  267.00  269.720  257.56  260.2  1427218.0            True   \n",
      "2024-03-27  267.00  269.720  257.56  260.2  1427218.0            True   \n",
      "2024-03-27  267.00  269.720  257.56  260.2  1427218.0            True   \n",
      "2024-03-28  258.53  268.545  258.00  263.9  1371668.0            True   \n",
      "2024-03-28  258.53  268.545  258.00  263.9  1371668.0            True   \n",
      "\n",
      "                                                     headline  \\\n",
      "date                                                            \n",
      "2024-03-27  Wall Street Lunch: Trump Media Could Double Wi...   \n",
      "2024-03-27  Spotify initiated, Lowe's downgraded: Wall Str...   \n",
      "2024-03-27  After betting $1 billion on podcasts, Spotify ...   \n",
      "2024-03-28                    Bull of the Day: Spotify (SPOT)   \n",
      "2024-03-28  Spotify to Add New Features for Universal Musi...   \n",
      "\n",
      "                                                      summary  daily_return  \\\n",
      "date                                                                          \n",
      "2024-03-27  Trump Media's shares are rallying, sparking di...           NaN   \n",
      "2024-03-27  Spotify initiated, Lowe's downgraded: Wall Str...      0.000000   \n",
      "2024-03-27  Users will fork out $117 for an introductory p...      0.000000   \n",
      "2024-03-28  Spotify's embrace of new media has propelled i...      1.421983   \n",
      "2024-03-28  By Mauro Orru Spotify Technology will introduc...      0.000000   \n",
      "\n",
      "            volatility_20d  abnormal_return  volume_change  log_volume  \\\n",
      "date                                                                     \n",
      "2024-03-27             NaN              NaN            NaN   14.171238   \n",
      "2024-03-27             NaN        -0.106240       0.000000   14.171238   \n",
      "2024-03-27             NaN        -0.106240       0.000000   14.171238   \n",
      "2024-03-28             NaN         1.315743      -3.892187   14.131538   \n",
      "2024-03-28             NaN        -0.106240       0.000000   14.131538   \n",
      "\n",
      "            headline_sentiment  summary_sentiment  combined_sentiment  \\\n",
      "date                                                                    \n",
      "2024-03-27              0.0000             0.5859             0.17577   \n",
      "2024-03-27              0.2023             0.2023             0.20230   \n",
      "2024-03-27              0.0000             0.0000             0.00000   \n",
      "2024-03-28              0.0000             0.3182             0.09546   \n",
      "2024-03-28              0.0000             0.8625             0.25875   \n",
      "\n",
      "           sentiment_category  \n",
      "date                           \n",
      "2024-03-27            neutral  \n",
      "2024-03-27           positive  \n",
      "2024-03-27            neutral  \n",
      "2024-03-28            neutral  \n",
      "2024-03-28           positive  \n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Aggregating data to daily level...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 248 entries, 2024-03-27 to 2025-02-28\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   headline_sentiment  248 non-null    float64\n",
      " 1   summary_sentiment   248 non-null    float64\n",
      " 2   combined_sentiment  248 non-null    float64\n",
      " 3   open                248 non-null    float64\n",
      " 4   high                248 non-null    float64\n",
      " 5   low                 248 non-null    float64\n",
      " 6   close               248 non-null    float64\n",
      " 7   volume              248 non-null    float64\n",
      " 8   daily_return        248 non-null    float64\n",
      " 9   volatility_20d      242 non-null    float64\n",
      " 10  volume_change       248 non-null    float64\n",
      " 11  abnormal_return     248 non-null    float64\n",
      " 12  log_volume          248 non-null    float64\n",
      " 13  is_trading_day      248 non-null    bool   \n",
      " 14  news_count          248 non-null    int64  \n",
      "dtypes: bool(1), float64(13), int64(1)\n",
      "memory usage: 29.3 KB\n",
      "None\n",
      "            headline_sentiment  summary_sentiment  combined_sentiment    open  \\\n",
      "date                                                                            \n",
      "2024-03-27            0.067433           0.262733            0.126023  267.00   \n",
      "2024-03-28            0.000000           0.541783            0.162535  258.53   \n",
      "2024-03-29            0.750600           0.102700            0.556230  258.53   \n",
      "2024-03-31            0.636900           0.888500            0.712380  258.53   \n",
      "2024-04-07            0.000000           0.361200            0.108360  298.68   \n",
      "\n",
      "               high     low   close     volume  daily_return  volatility_20d  \\\n",
      "date                                                                           \n",
      "2024-03-27  269.720  257.56  260.20  1427218.0      0.000000             NaN   \n",
      "2024-03-28  268.545  258.00  263.90  1371668.0      1.421983             NaN   \n",
      "2024-03-29  268.545  258.00  263.90  1371668.0      0.000000             NaN   \n",
      "2024-03-31  268.545  258.00  263.90  1371668.0      0.000000             NaN   \n",
      "2024-04-07  313.068  298.68  310.31  2988374.0     17.586207             NaN   \n",
      "\n",
      "            volume_change  abnormal_return  log_volume  is_trading_day  \\\n",
      "date                                                                     \n",
      "2024-03-27       0.000000        -0.106240   14.171238            True   \n",
      "2024-03-28      -3.892187         1.315743   14.131538            True   \n",
      "2024-03-29       0.000000        -0.106240   14.131538           False   \n",
      "2024-03-31       0.000000        -0.106240   14.131538           False   \n",
      "2024-04-07     117.864235        17.479967   14.910240           False   \n",
      "\n",
      "            news_count  \n",
      "date                    \n",
      "2024-03-27           3  \n",
      "2024-03-28           6  \n",
      "2024-03-29           1  \n",
      "2024-03-31           1  \n",
      "2024-04-07           1  \n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Creating lagged features up to 5 days...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 248 entries, 2024-03-27 to 2025-02-28\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   headline_sentiment  248 non-null    float64\n",
      " 1   summary_sentiment   248 non-null    float64\n",
      " 2   combined_sentiment  248 non-null    float64\n",
      " 3   open                248 non-null    float64\n",
      " 4   high                248 non-null    float64\n",
      " 5   low                 248 non-null    float64\n",
      " 6   close               248 non-null    float64\n",
      " 7   volume              248 non-null    float64\n",
      " 8   daily_return        248 non-null    float64\n",
      " 9   volatility_20d      242 non-null    float64\n",
      " 10  volume_change       248 non-null    float64\n",
      " 11  abnormal_return     248 non-null    float64\n",
      " 12  log_volume          248 non-null    float64\n",
      " 13  is_trading_day      248 non-null    bool   \n",
      " 14  news_count          248 non-null    int64  \n",
      " 15  sentiment_lag_1     247 non-null    float64\n",
      " 16  sentiment_lag_2     246 non-null    float64\n",
      " 17  sentiment_lag_3     245 non-null    float64\n",
      " 18  sentiment_lag_4     244 non-null    float64\n",
      " 19  sentiment_lag_5     243 non-null    float64\n",
      " 20  next_day_return     247 non-null    float64\n",
      "dtypes: bool(1), float64(19), int64(1)\n",
      "memory usage: 40.9 KB\n",
      "None\n",
      "            headline_sentiment  summary_sentiment  combined_sentiment    open  \\\n",
      "date                                                                            \n",
      "2024-03-27            0.067433           0.262733            0.126023  267.00   \n",
      "2024-03-28            0.000000           0.541783            0.162535  258.53   \n",
      "2024-03-29            0.750600           0.102700            0.556230  258.53   \n",
      "2024-03-31            0.636900           0.888500            0.712380  258.53   \n",
      "2024-04-07            0.000000           0.361200            0.108360  298.68   \n",
      "\n",
      "               high     low   close     volume  daily_return  volatility_20d  \\\n",
      "date                                                                           \n",
      "2024-03-27  269.720  257.56  260.20  1427218.0      0.000000             NaN   \n",
      "2024-03-28  268.545  258.00  263.90  1371668.0      1.421983             NaN   \n",
      "2024-03-29  268.545  258.00  263.90  1371668.0      0.000000             NaN   \n",
      "2024-03-31  268.545  258.00  263.90  1371668.0      0.000000             NaN   \n",
      "2024-04-07  313.068  298.68  310.31  2988374.0     17.586207             NaN   \n",
      "\n",
      "            ...  abnormal_return  log_volume  is_trading_day  news_count  \\\n",
      "date        ...                                                            \n",
      "2024-03-27  ...        -0.106240   14.171238            True           3   \n",
      "2024-03-28  ...         1.315743   14.131538            True           6   \n",
      "2024-03-29  ...        -0.106240   14.131538           False           1   \n",
      "2024-03-31  ...        -0.106240   14.131538           False           1   \n",
      "2024-04-07  ...        17.479967   14.910240           False           1   \n",
      "\n",
      "            sentiment_lag_1  sentiment_lag_2  sentiment_lag_3  \\\n",
      "date                                                            \n",
      "2024-03-27              NaN              NaN              NaN   \n",
      "2024-03-28         0.126023              NaN              NaN   \n",
      "2024-03-29         0.162535         0.126023              NaN   \n",
      "2024-03-31         0.556230         0.162535         0.126023   \n",
      "2024-04-07         0.712380         0.556230         0.162535   \n",
      "\n",
      "            sentiment_lag_4  sentiment_lag_5  next_day_return  \n",
      "date                                                           \n",
      "2024-03-27              NaN              NaN         1.421983  \n",
      "2024-03-28              NaN              NaN         0.000000  \n",
      "2024-03-29              NaN              NaN         0.000000  \n",
      "2024-03-31              NaN              NaN        17.586207  \n",
      "2024-04-07         0.126023              NaN        -0.399600  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Analyzing correlations between sentiment and stock metrics...\n",
      "\n",
      "Key correlation findings:\n",
      "  - Sentiment vs daily_return: r=-0.0439 (p=0.4912, not significant)\n",
      "  - Sentiment vs next_day_return: r=0.1268 (p=0.0464, significant)\n",
      "  - Sentiment vs volatility_20d: r=-0.0269 (p=0.6772, not significant)\n",
      "Analyzing lagged effects of sentiment on returns...\n",
      "Correlation with next day's return: 0.1268\n",
      "Building prediction model for stock returns...\n",
      "Model evaluation - Mean MSE: 7.1883, Mean R²: -0.4544\n",
      "Creating visualizations...\n",
      "\n",
      "Analysis completed! Key findings:\n",
      "1. Strongest lagged effect: sentiment_lag_1 (r=0.1268)\n",
      "2. Most important predictive features: sentiment_lag_1, volume_change, news_count\n",
      "3. Model prediction performance: R²=-0.4544\n"
     ]
    }
   ],
   "source": [
    "def run_sentiment_analysis(df_cleaned):\n",
    "    \"\"\"\n",
    "    Main function to run the complete sentiment analysis pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    df_cleaned (pandas.DataFrame): The cleaned Spotify stock and news data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Results of the analysis\n",
    "    \"\"\"\n",
    "    print(\"Starting Spotify stock and news sentiment analysis...\")\n",
    "    \n",
    "    # Step 1: Preprocess data\n",
    "    df_processed = preprocess_data(df_cleaned)\n",
    "    print(f\"Preprocessed data shape: {df_processed.shape}\")\n",
    "    print(df_processed.info())\n",
    "    print(df_processed.head())\n",
    "    print(\"\\n--------------------------------\\n\")\n",
    "    \n",
    "    # Step 2: Apply sentiment analysis\n",
    "    df_with_sentiment = analyze_sentiment(df_processed)\n",
    "    print(df_with_sentiment.info())\n",
    "    print(df_with_sentiment.head())\n",
    "    print(\"\\n--------------------------------\\n\")\n",
    "    \n",
    "    # Step 3: Aggregate to daily level\n",
    "    daily_data = aggregate_daily_data(df_with_sentiment)\n",
    "    print(daily_data.info())\n",
    "    print(daily_data.head())\n",
    "    print(\"\\n--------------------------------\\n\")\n",
    "    # Step 4: Create lagged features\n",
    "    daily_data_with_lags = create_lagged_features(daily_data)\n",
    "    print(daily_data_with_lags.info())\n",
    "    print(daily_data_with_lags.head())\n",
    "    print(\"\\n--------------------------------\\n\")\n",
    "    \n",
    "    # Step 5: Analyze correlation\n",
    "    corr_matrix, p_values = analyze_correlation(daily_data_with_lags)\n",
    "    \n",
    "    # Print key correlations\n",
    "    print(\"\\nKey correlation findings:\")\n",
    "    key_correlations = corr_matrix.loc['combined_sentiment', ['daily_return', 'next_day_return', 'volatility_20d']]\n",
    "    key_p_values = p_values.loc['combined_sentiment', ['daily_return', 'next_day_return', 'volatility_20d']]\n",
    "    for metric, corr in key_correlations.items():\n",
    "        p = key_p_values[metric]\n",
    "        significance = \"significant\" if p < 0.05 else \"not significant\"\n",
    "        print(f\"  - Sentiment vs {metric}: r={corr:.4f} (p={p:.4f}, {significance})\")\n",
    "    \n",
    "    # Step 6: Analyze lagged effects\n",
    "    lagged_corr = analyze_lagged_effects(daily_data_with_lags)\n",
    "    \n",
    "    # Step 7: Build prediction model\n",
    "    feature_importances, mse_scores, r2_scores = build_prediction_model(daily_data_with_lags)\n",
    "    \n",
    "    # Step 8: Visualize results\n",
    "    visualize_results(df_with_sentiment, daily_data, corr_matrix, lagged_corr, feature_importances)\n",
    "    \n",
    "    # Step 9: Prepare summary of findings\n",
    "    print(\"\\nAnalysis completed! Key findings:\")\n",
    "    \n",
    "    # Determine most influential sentiment lag\n",
    "    max_lag_idx = lagged_corr.abs().idxmax()\n",
    "    max_lag_corr = lagged_corr[max_lag_idx]\n",
    "    max_lag = int(max_lag_idx.split('_')[-1])\n",
    "    \n",
    "    print(f\"1. Strongest lagged effect: {max_lag_idx} (r={max_lag_corr:.4f})\")\n",
    "    print(f\"2. Most important predictive features: {', '.join(feature_importances['feature'].head(3).tolist())}\")\n",
    "    print(f\"3. Model prediction performance: R²={np.mean(r2_scores):.4f}\")\n",
    "    \n",
    "    # Return results for further analysis if needed\n",
    "    results = {\n",
    "        'df_with_sentiment': df_with_sentiment,\n",
    "        'daily_data': daily_data,\n",
    "        'daily_data_with_lags': daily_data_with_lags,\n",
    "        'corr_matrix': corr_matrix,\n",
    "        'p_values': p_values,\n",
    "        'lagged_corr': lagged_corr,\n",
    "        'feature_importances': feature_importances,\n",
    "        'mse_scores': mse_scores,\n",
    "        'r2_scores': r2_scores\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute the analysis\n",
    "results = run_sentiment_analysis(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "### The Question: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: \n",
    "What does the sentiment analysis of news articles about Spotify reveal about the relationship between media coverage and stock performance, and what are the key findings regarding prediction capabilities?\n",
    "\n",
    "# Answer:\n",
    "\n",
    "## Introduction to the Spotify News Sentiment Analysis\n",
    "\n",
    "The analysis I conducted examined the relationship between news sentiment and Spotify's stock performance over a one-year period. By applying natural language processing techniques to headlines and article summaries while correlating them with stock metrics, I sought to determine whether news sentiment can predict stock movements.\n",
    "\n",
    "## Dataset and Methodology\n",
    "\n",
    "The dataset contained 879 unique news articles about Spotify spanning from March 2024 to February 2025, along with daily stock data including:\n",
    "- Opening, closing, high, and low prices\n",
    "- Trading volume \n",
    "- Boolean indicators for trading days\n",
    "\n",
    "The methodology involved several key steps:\n",
    "\n",
    "1. **Preprocessing**: Calculated financial metrics including daily returns, 20-day volatility, abnormal returns, and volume changes.\n",
    "\n",
    "2. **Sentiment Analysis**: Applied VADER (Valence Aware Dictionary and sEntiment Reasoner) to evaluate sentiment in headlines and summaries, with a weighted approach (70% headline, 30% summary) to create a combined sentiment score.\n",
    "\n",
    "3. **Aggregation**: News articles were aggregated at the daily level, averaging sentiment scores when multiple articles appeared on the same day.\n",
    "\n",
    "4. **Lagged Features**: Created lagged sentiment variables to test whether past sentiment predicted future returns.\n",
    "\n",
    "5. **Correlation Analysis**: Examined relationships between sentiment and stock metrics with statistical significance testing.\n",
    "\n",
    "6. **Predictive Modeling**: Built a Random Forest model to evaluate sentiment's predictive power for next-day returns.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Sentiment and Same-Day Returns\n",
    "The analysis revealed no significant correlation between news sentiment and same-day stock returns (r=-0.0439, p=0.4912). This suggests that on the day news is published, there is no consistent relationship between the sentiment of the news and Spotify's stock performance.\n",
    "\n",
    "### 2. Sentiment and Next-Day Returns\n",
    "Interestingly, there was a statistically significant correlation between combined sentiment and next-day returns (r=0.1268, p=0.0464). This suggests a small but meaningful predictive relationship where positive news sentiment tends to precede positive stock movements the following day.\n",
    "\n",
    "### 3. Lagged Sentiment Effects\n",
    "The lag-1 sentiment (previous day's sentiment) showed the strongest relationship with current day returns, reinforcing the finding that news sentiment may have a delayed effect on stock performance.\n",
    "\n",
    "### 4. Volatility and News Volume\n",
    "The analysis found no significant correlation between sentiment and volatility (r=-0.0269, p=0.6772), suggesting that news sentiment does not consistently affect the magnitude of price movements.\n",
    "\n",
    "### 5. Predictive Modeling Performance\n",
    "Despite the statistically significant correlation with next-day returns, the Random Forest prediction model performed poorly with a negative R² value (-0.4544). This indicates that while there is a statistically significant relationship, it's not strong enough to build a reliable prediction model with the features used.\n",
    "\n",
    "### 6. Feature Importance\n",
    "The most important predictive features identified were:\n",
    "- Previous day's sentiment (sentiment_lag_1)\n",
    "- Volume change\n",
    "- Number of news articles (news_count)\n",
    "\n",
    "## Visual Analysis\n",
    "\n",
    "The visualizations (shown in the images) revealed several additional insights:\n",
    "\n",
    "1. **Stock Price and Sentiment Trends**: The time series plot showed considerable fluctuation in sentiment over time with no obvious pattern matching stock price movements.\n",
    "\n",
    "2. **Sentiment Categories and Returns**: The boxplot of returns by sentiment category showed minimal differences between negative, neutral, and positive sentiment groups, confirmed by a non-significant ANOVA test.\n",
    "\n",
    "3. **Feature Importance**: The feature importance chart highlighted the prominence of sentiment_lag_1, confirming the lagged effect finding.\n",
    "\n",
    "4. **News Volume and Volatility**: The scatter plot revealed some clustering of higher volatility with higher news counts, suggesting increased media attention during more volatile periods.\n",
    "\n",
    "## Conclusions and Implications\n",
    "\n",
    "1. **Delayed Impact**: News sentiment appears to have a delayed impact on Spotify's stock, with previous day's sentiment showing the strongest relationship with current day returns.\n",
    "\n",
    "2. **Statistical vs. Practical Significance**: While the correlation between sentiment and next-day returns is statistically significant, its practical value for prediction is limited, as evidenced by the negative R² value.\n",
    "\n",
    "3. **Complex Relationships**: The analysis suggests that the relationship between news sentiment and stock performance is complex and likely influenced by many other factors not captured in this analysis.\n",
    "\n",
    "4. **Trading Strategy Limitations**: The findings indicate that a simple trading strategy based solely on news sentiment would likely not be profitable, given the weak predictive power of the model.\n",
    "\n",
    "5. **News Volume Matters**: The prominence of news_count as an important feature suggests that the amount of media coverage may be as important as its sentiment in understanding stock behavior.\n",
    "\n",
    "## Future Research Directions\n",
    "\n",
    "To build on these findings, future research could:\n",
    "\n",
    "1. Incorporate market-wide factors to better isolate Spotify-specific effects\n",
    "2. Analyze sentiment within specific news categories (earnings reports, product launches, etc.)\n",
    "3. Apply more sophisticated NLP techniques beyond VADER sentiment analysis\n",
    "4. Expand the timeframe to capture longer-term trends and seasonal patterns\n",
    "5. Include social media sentiment alongside traditional news sources\n",
    "\n",
    "This analysis provides evidence of a statistically significant but practically limited relationship between news sentiment and Spotify's stock performance, with the most notable finding being the delayed effect of sentiment on stock returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
