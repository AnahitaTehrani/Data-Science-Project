{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Dataset Preparation\n",
    "\n",
    "The Dataset of Spotify Stock News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  category    datetime                                           headline  \\\n",
      "0  company  1740759981  Palantir, Nvidia Nixed But Netflix Remains On ...   \n",
      "1  company  1740684439  Tracking Chase Coleman's Tiger Global Portfoli...   \n",
      "2  company  1740664816  Spotify Technology (SPOT) is Attracting Invest...   \n",
      "3  company  1740658999  YouTube Surpasses Competitors in Streaming and...   \n",
      "4  company  1740593693  Spotify CEO Wants EU To Penalize Apple For Def...   \n",
      "\n",
      "          id                                              image related  \\\n",
      "0  132937882  https://media.zenfs.com/en/ibd.com/fc3f416bcfc...    SPOT   \n",
      "1  132901707  https://static.seekingalpha.com/cdn/s3/uploads...    SPOT   \n",
      "2  132937883  https://media.zenfs.com/en/zacks.com/bdc2850a4...    SPOT   \n",
      "3  132937884  https://media.zenfs.com/en/us.finance.gurufocu...    SPOT   \n",
      "4  132937885  https://media.zenfs.com/en/Benzinga/20dc68a2c9...    SPOT   \n",
      "\n",
      "         source                                            summary  \\\n",
      "0         Yahoo  A volatile market has shaken off Nvidia and Pa...   \n",
      "1  SeekingAlpha  Tiger Global's 13F reveals a $26.46B portfolio...   \n",
      "2         Yahoo  Spotify (SPOT) has been one of the stocks most...   \n",
      "3         Yahoo  Nielsen ranks YouTube ahead of Netflix and Hul...   \n",
      "4         Yahoo  Spotify Technology’s (NYSE:SPOT) CEO, Daniel E...   \n",
      "\n",
      "                                                 url        date  \n",
      "0  https://finnhub.io/api/news?id=578a14cc0987e6c...  2025-02-28  \n",
      "1  https://finnhub.io/api/news?id=c7d354e4ade5e63...  2025-02-27  \n",
      "2  https://finnhub.io/api/news?id=cec976f64d52fff...  2025-02-27  \n",
      "3  https://finnhub.io/api/news?id=1fd8ee4da1b2b5e...  2025-02-27  \n",
      "4  https://finnhub.io/api/news?id=e4ed9eb39665a16...  2025-02-26  \n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "     open      high      low   close     volume  adj_high  adj_low  adj_close  \\\n",
      "0  584.25  609.9200  580.000  608.01  4531895.0  609.9200  580.000     608.01   \n",
      "1  611.00  613.0000  586.000  590.76  1191837.0  613.0000  586.000     590.76   \n",
      "2  595.62  608.5294  592.890  603.13  2629946.0  608.5294  592.890     603.13   \n",
      "3  597.22  599.1200  575.535  588.57  2822820.0  599.1200  575.535     588.57   \n",
      "4  612.30  621.9100  592.980  601.61  2078629.0  621.9100  592.980     601.61   \n",
      "\n",
      "   adj_open  adj_volume  split_factor  dividend symbol exchange        date  \n",
      "0    584.25   4531895.0           1.0       0.0   SPOT     XNYS  2025-02-28  \n",
      "1    611.00   1191837.0           1.0       0.0   SPOT     XNYS  2025-02-27  \n",
      "2    595.62   2629946.0           1.0       0.0   SPOT     XNYS  2025-02-26  \n",
      "3    597.22   2822820.0           1.0       0.0   SPOT     XNYS  2025-02-25  \n",
      "4    612.30   2078629.0           1.0       0.0   SPOT     XNYS  2025-02-24  \n"
     ]
    }
   ],
   "source": [
    "#load the csv file that contains the news from the finhub api 03/2024 to 02/2025\n",
    "df_news = pd.read_csv(\"/Users/armandocriscuolo/c2025/data_science_project_2025/code/Data-Science-Project/Data-Question-2-B/news_analysis/spotify_news_2024_2025_finhub.csv\")\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_news.head())\n",
    "\n",
    "print(\"\\n\\n-----------------------------------\\n\\n\")\n",
    "\n",
    "#load the csv file that contains the stock data from the marketstack api 03/2024 to 02/2025\n",
    "df_stock = pd.read_csv(\"/Users/armandocriscuolo/c2025/data_science_project_2025/code/Data-Science-Project/Data-Question-2-B/news_analysis/spotify_stock_data_20250323_193740.csv\")\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_stock.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spotify Stock Data\n",
      "         date    open      high      low   close     volume\n",
      "0  2025-02-28  584.25  609.9200  580.000  608.01  4531895.0\n",
      "1  2025-02-27  611.00  613.0000  586.000  590.76  1191837.0\n",
      "2  2025-02-26  595.62  608.5294  592.890  603.13  2629946.0\n",
      "3  2025-02-25  597.22  599.1200  575.535  588.57  2822820.0\n",
      "4  2025-02-24  612.30  621.9100  592.980  601.61  2078629.0\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "Spotify News Data\n",
      "         date                                           headline  \\\n",
      "0  2025-02-28  Palantir, Nvidia Nixed But Netflix Remains On ...   \n",
      "1  2025-02-27  Tracking Chase Coleman's Tiger Global Portfoli...   \n",
      "2  2025-02-27  Spotify Technology (SPOT) is Attracting Invest...   \n",
      "3  2025-02-27  YouTube Surpasses Competitors in Streaming and...   \n",
      "4  2025-02-26  Spotify CEO Wants EU To Penalize Apple For Def...   \n",
      "\n",
      "                                             summary  \n",
      "0  A volatile market has shaken off Nvidia and Pa...  \n",
      "1  Tiger Global's 13F reveals a $26.46B portfolio...  \n",
      "2  Spotify (SPOT) has been one of the stocks most...  \n",
      "3  Nielsen ranks YouTube ahead of Netflix and Hul...  \n",
      "4  Spotify Technology’s (NYSE:SPOT) CEO, Daniel E...  \n"
     ]
    }
   ],
   "source": [
    "#Lets clean the dataframe to have only the columns we need\n",
    "print(\"Spotify Stock Data\")\n",
    "df_stock = df_stock[[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_stock.head())\n",
    "\n",
    "print(\"\\n\\n-----------------------------------\\n\\n\")\n",
    "\n",
    "print(\"Spotify News Data\")\n",
    "#lets clean the dataframe to have only the columns we need\n",
    "df_news = df_news[[\"date\", \"headline\", \"summary\"]]\n",
    "\n",
    "#print the first 5 rows of the dataframe\n",
    "print(df_news.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1444 entries, 0 to 1443\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            1444 non-null   datetime64[ns]\n",
      " 1   open            1444 non-null   float64       \n",
      " 2   high            1444 non-null   float64       \n",
      " 3   low             1444 non-null   float64       \n",
      " 4   close           1444 non-null   float64       \n",
      " 5   volume          1444 non-null   float64       \n",
      " 6   is_trading_day  1444 non-null   bool          \n",
      " 7   headline        1444 non-null   object        \n",
      " 8   summary         1444 non-null   object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 102.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert dates to datetime format\n",
    "df_stock['date'] = pd.to_datetime(df_stock['date'])\n",
    "df_news['date'] = pd.to_datetime(df_news['date'])\n",
    "\n",
    "# Create complete date range DataFrame\n",
    "start_date = min(df_stock['date'].min(), df_news['date'].min())\n",
    "end_date = max(df_stock['date'].max(), df_news['date'].max())\n",
    "date_range = pd.DataFrame({'date': pd.date_range(start=start_date, end=end_date)})\n",
    "\n",
    "# Merge stock data with complete date range and forward fill missing values\n",
    "complete_stock = pd.merge(date_range, df_stock, on='date', how='left')\n",
    "complete_stock = complete_stock.sort_values('date')\n",
    "complete_stock = complete_stock.ffill()  # Forward fill to use previous day's data for missing dates\n",
    "\n",
    "# Add indicator for trading days\n",
    "complete_stock['is_trading_day'] = complete_stock['date'].isin(df_stock['date'])\n",
    "\n",
    "# Now merge with news data, keeping one row per news item\n",
    "df_merged = pd.merge(complete_stock, df_news, on='date', how='outer')\n",
    "\n",
    "# Sort by date\n",
    "df_merged = df_merged.sort_values('date')\n",
    "\n",
    "# Fill NaN values in news columns\n",
    "df_merged['headline'] = df_merged['headline'].fillna('')\n",
    "df_merged['summary'] = df_merged['summary'].fillna('')\n",
    "\n",
    "print(df_merged.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicated headlines: 32\n",
      "\n",
      "Top 10 most duplicated headlines:\n",
      "headline\n",
      "                                                                                            73\n",
      "Spotify Technology (SPOT) is Attracting Investor Attention: Here is What You Should Know     5\n",
      "What You Missed On Wall Street This Morning                                                  4\n",
      "Spotify Technology (SPOT) Is a Trending Stock: Facts to Know Before Betting on It            4\n",
      "Is Trending Stock Spotify Technology (SPOT) a Buy Now?                                       3\n",
      "What You Missed On Wall Street On Tuesday                                                    3\n",
      "Morgan Stanley Reaffirms Their Buy Rating on Spotify Technology SA (SPOT)                    3\n",
      "Spotify Technology S.A. (SPOT): A Bull Case Theory                                           3\n",
      "A Closer Look at Spotify Technology's Options Market Dynamics                                3\n",
      "Spotify price target raised by $50 at KeyBanc, here's why                                    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Total number of duplicated summaries: 18\n",
      "\n",
      "Top 10 most duplicated summaries:\n",
      "summary\n",
      "Looking for stock market analysis and research with proves results? Zacks.com offers in-depth financial research with over 30years of proven results.                                                                                                                 432\n",
      "                                                                                                                                                                                                                                                                       92\n",
      "Recently, Zacks.com users have been paying close attention to Spotify (SPOT). This makes it worthwhile to examine what the stock has in store.                                                                                                                          7\n",
      "The average brokerage recommendation (ABR) for Spotify (SPOT) is equivalent to a Buy. The overly optimistic recommendations of Wall Street analysts make the effectiveness of this highly sought-after metric questionable. So, is it worth buying the stock?           6\n",
      "Spotify (SPOT) has been one of the stocks most watched by Zacks.com users lately. So, it is worth exploring what lies ahead for the stock.                                                                                                                              4\n",
      "Based on the average brokerage recommendation (ABR), Spotify (SPOT) should be added to one's portfolio. Wall Street analysts' overly optimistic recommendations cast doubt on the effectiveness of this highly sought-after metric. So, is the stock worth buying?      4\n",
      "Spotify (SPOT) shares have started gaining and might continue moving higher in the near term, as indicated by solid earnings estimate revisions.                                                                                                                        3\n",
      "Spotify (SPOT) doesn't possess the right combination of the two key ingredients for a likely earnings beat in its upcoming report. Get prepared with the key expectations.                                                                                              3\n",
      "Spotify (SPOT) has received quite a bit of attention from Zacks.com users lately. Therefore, it is wise to be aware of the facts that can impact the stock's prospects.                                                                                                 3\n",
      "Document UNITED STATES SECURITIES AND EXCHANGE...                                                                                                                                                                                                                       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated headline:\n",
      "Headline (appearing 73 times): \n",
      "\n",
      "Dates when this headline appeared:\n",
      "- 2024-03-25 00:00:00\n",
      "- 2024-03-26 00:00:00\n",
      "- 2024-04-01 00:00:00\n",
      "- 2024-04-02 00:00:00\n",
      "- 2024-04-03 00:00:00\n",
      "- 2024-04-04 00:00:00\n",
      "- 2024-04-05 00:00:00\n",
      "- 2024-04-06 00:00:00\n",
      "- 2024-05-04 00:00:00\n",
      "- 2024-05-18 00:00:00\n",
      "- 2024-05-19 00:00:00\n",
      "- 2024-05-20 00:00:00\n",
      "- 2024-05-25 00:00:00\n",
      "- 2024-05-27 00:00:00\n",
      "- 2024-06-01 00:00:00\n",
      "- 2024-06-02 00:00:00\n",
      "- 2024-06-08 00:00:00\n",
      "- 2024-06-30 00:00:00\n",
      "- 2024-07-04 00:00:00\n",
      "- 2024-07-06 00:00:00\n",
      "- 2024-07-07 00:00:00\n",
      "- 2024-08-03 00:00:00\n",
      "- 2024-08-04 00:00:00\n",
      "- 2024-08-11 00:00:00\n",
      "- 2024-08-24 00:00:00\n",
      "- 2024-09-01 00:00:00\n",
      "- 2024-09-14 00:00:00\n",
      "- 2024-09-15 00:00:00\n",
      "- 2024-09-18 00:00:00\n",
      "- 2024-09-21 00:00:00\n",
      "- 2024-09-22 00:00:00\n",
      "- 2024-09-27 00:00:00\n",
      "- 2024-09-28 00:00:00\n",
      "- 2024-10-05 00:00:00\n",
      "- 2024-10-12 00:00:00\n",
      "- 2024-10-13 00:00:00\n",
      "- 2024-10-20 00:00:00\n",
      "- 2024-11-02 00:00:00\n",
      "- 2024-11-03 00:00:00\n",
      "- 2024-11-04 00:00:00\n",
      "- 2024-11-17 00:00:00\n",
      "- 2024-11-24 00:00:00\n",
      "- 2024-11-30 00:00:00\n",
      "- 2024-12-02 00:00:00\n",
      "- 2024-12-07 00:00:00\n",
      "- 2024-12-13 00:00:00\n",
      "- 2024-12-16 00:00:00\n",
      "- 2024-12-21 00:00:00\n",
      "- 2024-12-25 00:00:00\n",
      "- 2024-12-29 00:00:00\n",
      "- 2025-01-05 00:00:00\n",
      "- 2025-01-09 00:00:00\n",
      "- 2025-01-11 00:00:00\n",
      "- 2025-01-12 00:00:00\n",
      "- 2025-01-13 00:00:00\n",
      "- 2025-01-16 00:00:00\n",
      "- 2025-01-17 00:00:00\n",
      "- 2025-01-19 00:00:00\n",
      "- 2025-01-23 00:00:00\n",
      "- 2025-01-24 00:00:00\n",
      "- 2025-01-25 00:00:00\n",
      "- 2025-01-29 00:00:00\n",
      "- 2025-02-01 00:00:00\n",
      "- 2025-02-02 00:00:00\n",
      "- 2025-02-03 00:00:00\n",
      "- 2025-02-06 00:00:00\n",
      "- 2025-02-08 00:00:00\n",
      "- 2025-02-09 00:00:00\n",
      "- 2025-02-12 00:00:00\n",
      "- 2025-02-15 00:00:00\n",
      "- 2025-02-16 00:00:00\n",
      "- 2025-02-22 00:00:00\n",
      "- 2025-02-23 00:00:00\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated summary:\n",
      "Summary (appearing 432 times): Looking for stock market analysis and research with proves results? Zacks.com offers in-depth financial research with over 30years of proven results.\n",
      "\n",
      "Dates when this summary appeared:\n",
      "- 2024-03-27 00:00:00\n",
      "- 2024-03-27 00:00:00\n",
      "- 2024-03-27 00:00:00\n",
      "- 2024-03-27 00:00:00\n",
      "- 2024-03-27 00:00:00\n",
      "- 2024-03-27 00:00:00\n",
      "- 2024-03-27 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-28 00:00:00\n",
      "- 2024-03-30 00:00:00\n",
      "- 2024-04-08 00:00:00\n",
      "- 2024-04-08 00:00:00\n",
      "- 2024-04-08 00:00:00\n",
      "- 2024-04-08 00:00:00\n",
      "- 2024-04-08 00:00:00\n",
      "- 2024-04-08 00:00:00\n",
      "- 2024-04-09 00:00:00\n",
      "- 2024-04-10 00:00:00\n",
      "- 2024-04-10 00:00:00\n",
      "- 2024-04-10 00:00:00\n",
      "- 2024-04-10 00:00:00\n",
      "- 2024-04-11 00:00:00\n",
      "- 2024-04-11 00:00:00\n",
      "- 2024-04-11 00:00:00\n",
      "- 2024-04-11 00:00:00\n",
      "- 2024-04-12 00:00:00\n",
      "- 2024-04-12 00:00:00\n",
      "- 2024-04-12 00:00:00\n",
      "- 2024-04-12 00:00:00\n",
      "- 2024-04-12 00:00:00\n",
      "- 2024-04-12 00:00:00\n",
      "- 2024-04-13 00:00:00\n",
      "- 2024-04-14 00:00:00\n",
      "- 2024-04-15 00:00:00\n",
      "- 2024-04-16 00:00:00\n",
      "- 2024-04-17 00:00:00\n",
      "- 2024-04-17 00:00:00\n",
      "- 2024-04-17 00:00:00\n",
      "- 2024-04-18 00:00:00\n",
      "- 2024-04-18 00:00:00\n",
      "- 2024-04-18 00:00:00\n",
      "- 2024-04-19 00:00:00\n",
      "- 2024-04-19 00:00:00\n",
      "- 2024-04-19 00:00:00\n",
      "- 2024-04-19 00:00:00\n",
      "- 2024-04-21 00:00:00\n",
      "- 2024-04-21 00:00:00\n",
      "- 2024-04-21 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-22 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-23 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-24 00:00:00\n",
      "- 2024-04-25 00:00:00\n",
      "- 2024-04-25 00:00:00\n",
      "- 2024-04-25 00:00:00\n",
      "- 2024-04-26 00:00:00\n",
      "- 2024-04-26 00:00:00\n",
      "- 2024-04-26 00:00:00\n",
      "- 2024-04-26 00:00:00\n",
      "- 2024-04-26 00:00:00\n",
      "- 2024-04-28 00:00:00\n",
      "- 2024-04-28 00:00:00\n",
      "- 2024-04-29 00:00:00\n",
      "- 2024-04-29 00:00:00\n",
      "- 2024-04-29 00:00:00\n",
      "- 2024-05-01 00:00:00\n",
      "- 2024-05-01 00:00:00\n",
      "- 2024-05-01 00:00:00\n",
      "- 2024-05-02 00:00:00\n",
      "- 2024-05-03 00:00:00\n",
      "- 2024-05-05 00:00:00\n",
      "- 2024-05-06 00:00:00\n",
      "- 2024-05-07 00:00:00\n",
      "- 2024-05-07 00:00:00\n",
      "- 2024-05-08 00:00:00\n",
      "- 2024-05-09 00:00:00\n",
      "- 2024-05-09 00:00:00\n",
      "- 2024-05-09 00:00:00\n",
      "- 2024-05-09 00:00:00\n",
      "- 2024-05-10 00:00:00\n",
      "- 2024-05-10 00:00:00\n",
      "- 2024-05-10 00:00:00\n",
      "- 2024-05-12 00:00:00\n",
      "- 2024-05-14 00:00:00\n",
      "- 2024-05-15 00:00:00\n",
      "- 2024-05-15 00:00:00\n",
      "- 2024-05-16 00:00:00\n",
      "- 2024-05-16 00:00:00\n",
      "- 2024-05-17 00:00:00\n",
      "- 2024-05-21 00:00:00\n",
      "- 2024-05-21 00:00:00\n",
      "- 2024-05-21 00:00:00\n",
      "- 2024-05-22 00:00:00\n",
      "- 2024-05-22 00:00:00\n",
      "- 2024-05-22 00:00:00\n",
      "- 2024-05-22 00:00:00\n",
      "- 2024-05-23 00:00:00\n",
      "- 2024-05-24 00:00:00\n",
      "- 2024-05-24 00:00:00\n",
      "- 2024-05-24 00:00:00\n",
      "- 2024-05-26 00:00:00\n",
      "- 2024-05-28 00:00:00\n",
      "- 2024-05-29 00:00:00\n",
      "- 2024-05-29 00:00:00\n",
      "- 2024-05-30 00:00:00\n",
      "- 2024-05-31 00:00:00\n",
      "- 2024-05-31 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-03 00:00:00\n",
      "- 2024-06-04 00:00:00\n",
      "- 2024-06-04 00:00:00\n",
      "- 2024-06-04 00:00:00\n",
      "- 2024-06-04 00:00:00\n",
      "- 2024-06-04 00:00:00\n",
      "- 2024-06-04 00:00:00\n",
      "- 2024-06-04 00:00:00\n",
      "- 2024-06-05 00:00:00\n",
      "- 2024-06-06 00:00:00\n",
      "- 2024-06-06 00:00:00\n",
      "- 2024-06-07 00:00:00\n",
      "- 2024-06-07 00:00:00\n",
      "- 2024-06-07 00:00:00\n",
      "- 2024-06-07 00:00:00\n",
      "- 2024-06-11 00:00:00\n",
      "- 2024-06-11 00:00:00\n",
      "- 2024-06-11 00:00:00\n",
      "- 2024-06-11 00:00:00\n",
      "- 2024-06-11 00:00:00\n",
      "- 2024-06-11 00:00:00\n",
      "- 2024-06-13 00:00:00\n",
      "- 2024-06-13 00:00:00\n",
      "- 2024-06-13 00:00:00\n",
      "- 2024-06-14 00:00:00\n",
      "- 2024-06-14 00:00:00\n",
      "- 2024-06-15 00:00:00\n",
      "- 2024-06-17 00:00:00\n",
      "- 2024-06-18 00:00:00\n",
      "- 2024-06-19 00:00:00\n",
      "- 2024-06-19 00:00:00\n",
      "- 2024-06-20 00:00:00\n",
      "- 2024-06-21 00:00:00\n",
      "- 2024-06-22 00:00:00\n",
      "- 2024-06-23 00:00:00\n",
      "- 2024-06-24 00:00:00\n",
      "- 2024-06-26 00:00:00\n",
      "- 2024-06-26 00:00:00\n",
      "- 2024-06-27 00:00:00\n",
      "- 2024-06-28 00:00:00\n",
      "- 2024-07-17 00:00:00\n",
      "- 2024-07-18 00:00:00\n",
      "- 2024-07-19 00:00:00\n",
      "- 2024-07-21 00:00:00\n",
      "- 2024-07-21 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-22 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-23 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-24 00:00:00\n",
      "- 2024-07-25 00:00:00\n",
      "- 2024-07-25 00:00:00\n",
      "- 2024-07-26 00:00:00\n",
      "- 2024-07-26 00:00:00\n",
      "- 2024-07-26 00:00:00\n",
      "- 2024-07-26 00:00:00\n",
      "- 2024-07-26 00:00:00\n",
      "- 2024-07-27 00:00:00\n",
      "- 2024-07-29 00:00:00\n",
      "- 2024-07-30 00:00:00\n",
      "- 2024-07-30 00:00:00\n",
      "- 2024-07-30 00:00:00\n",
      "- 2024-07-30 00:00:00\n",
      "- 2024-07-31 00:00:00\n",
      "- 2024-07-31 00:00:00\n",
      "- 2024-07-31 00:00:00\n",
      "- 2024-08-01 00:00:00\n",
      "- 2024-08-01 00:00:00\n",
      "- 2024-08-01 00:00:00\n",
      "- 2024-08-01 00:00:00\n",
      "- 2024-08-05 00:00:00\n",
      "- 2024-08-05 00:00:00\n",
      "- 2024-08-05 00:00:00\n",
      "- 2024-08-05 00:00:00\n",
      "- 2024-08-07 00:00:00\n",
      "- 2024-08-08 00:00:00\n",
      "- 2024-08-08 00:00:00\n",
      "- 2024-08-09 00:00:00\n",
      "- 2024-08-12 00:00:00\n",
      "- 2024-08-12 00:00:00\n",
      "- 2024-08-13 00:00:00\n",
      "- 2024-08-14 00:00:00\n",
      "- 2024-08-14 00:00:00\n",
      "- 2024-08-14 00:00:00\n",
      "- 2024-08-14 00:00:00\n",
      "- 2024-08-14 00:00:00\n",
      "- 2024-08-14 00:00:00\n",
      "- 2024-08-14 00:00:00\n",
      "- 2024-08-15 00:00:00\n",
      "- 2024-08-16 00:00:00\n",
      "- 2024-08-18 00:00:00\n",
      "- 2024-08-19 00:00:00\n",
      "- 2024-08-21 00:00:00\n",
      "- 2024-08-22 00:00:00\n",
      "- 2024-08-22 00:00:00\n",
      "- 2024-08-23 00:00:00\n",
      "- 2024-08-26 00:00:00\n",
      "- 2024-08-27 00:00:00\n",
      "- 2024-08-27 00:00:00\n",
      "- 2024-08-27 00:00:00\n",
      "- 2024-08-28 00:00:00\n",
      "- 2024-08-28 00:00:00\n",
      "- 2024-09-04 00:00:00\n",
      "- 2024-09-04 00:00:00\n",
      "- 2024-09-05 00:00:00\n",
      "- 2024-09-05 00:00:00\n",
      "- 2024-09-05 00:00:00\n",
      "- 2024-09-05 00:00:00\n",
      "- 2024-09-06 00:00:00\n",
      "- 2024-09-06 00:00:00\n",
      "- 2024-09-06 00:00:00\n",
      "- 2024-09-10 00:00:00\n",
      "- 2024-09-10 00:00:00\n",
      "- 2024-09-11 00:00:00\n",
      "- 2024-09-11 00:00:00\n",
      "- 2024-09-12 00:00:00\n",
      "- 2024-09-16 00:00:00\n",
      "- 2024-09-16 00:00:00\n",
      "- 2024-09-16 00:00:00\n",
      "- 2024-09-16 00:00:00\n",
      "- 2024-09-16 00:00:00\n",
      "- 2024-09-16 00:00:00\n",
      "- 2024-09-16 00:00:00\n",
      "- 2024-09-19 00:00:00\n",
      "- 2024-09-19 00:00:00\n",
      "- 2024-09-19 00:00:00\n",
      "- 2024-09-19 00:00:00\n",
      "- 2024-09-19 00:00:00\n",
      "- 2024-09-19 00:00:00\n",
      "- 2024-09-23 00:00:00\n",
      "- 2024-09-23 00:00:00\n",
      "- 2024-09-24 00:00:00\n",
      "- 2024-09-24 00:00:00\n",
      "- 2024-09-25 00:00:00\n",
      "- 2024-09-26 00:00:00\n",
      "- 2024-09-26 00:00:00\n",
      "- 2024-09-29 00:00:00\n",
      "- 2024-09-29 00:00:00\n",
      "- 2024-09-29 00:00:00\n",
      "- 2024-09-30 00:00:00\n",
      "- 2024-09-30 00:00:00\n",
      "- 2024-09-30 00:00:00\n",
      "- 2024-10-01 00:00:00\n",
      "- 2024-10-02 00:00:00\n",
      "- 2024-10-02 00:00:00\n",
      "- 2024-10-02 00:00:00\n",
      "- 2024-10-02 00:00:00\n",
      "- 2024-10-02 00:00:00\n",
      "- 2024-10-02 00:00:00\n",
      "- 2024-10-03 00:00:00\n",
      "- 2024-10-04 00:00:00\n",
      "- 2024-10-04 00:00:00\n",
      "- 2024-10-04 00:00:00\n",
      "- 2024-10-07 00:00:00\n",
      "- 2024-10-09 00:00:00\n",
      "- 2024-10-09 00:00:00\n",
      "- 2024-10-09 00:00:00\n",
      "- 2024-10-10 00:00:00\n",
      "- 2024-10-10 00:00:00\n",
      "- 2024-10-10 00:00:00\n",
      "- 2024-10-11 00:00:00\n",
      "- 2024-10-11 00:00:00\n",
      "- 2024-10-14 00:00:00\n",
      "- 2024-10-14 00:00:00\n",
      "- 2024-10-14 00:00:00\n",
      "- 2024-10-14 00:00:00\n",
      "- 2024-10-15 00:00:00\n",
      "- 2024-10-16 00:00:00\n",
      "- 2024-10-16 00:00:00\n",
      "- 2024-10-16 00:00:00\n",
      "- 2024-10-17 00:00:00\n",
      "- 2024-10-17 00:00:00\n",
      "- 2024-10-19 00:00:00\n",
      "- 2024-10-21 00:00:00\n",
      "- 2024-10-21 00:00:00\n",
      "- 2024-10-22 00:00:00\n",
      "- 2024-10-22 00:00:00\n",
      "- 2024-10-23 00:00:00\n",
      "- 2024-10-23 00:00:00\n",
      "- 2024-10-25 00:00:00\n",
      "- 2024-10-25 00:00:00\n",
      "- 2024-10-26 00:00:00\n",
      "- 2024-10-28 00:00:00\n",
      "- 2024-10-28 00:00:00\n",
      "- 2024-10-28 00:00:00\n",
      "- 2024-10-28 00:00:00\n",
      "- 2024-10-28 00:00:00\n",
      "- 2024-10-28 00:00:00\n",
      "- 2024-10-29 00:00:00\n",
      "- 2024-10-30 00:00:00\n",
      "- 2024-10-30 00:00:00\n",
      "- 2024-10-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Count duplicated headlines and summaries\n",
    "headline_counts = df_merged['headline'].value_counts()\n",
    "duplicated_headlines = headline_counts[headline_counts > 1]\n",
    "print(f\"Total number of duplicated headlines: {len(duplicated_headlines)}\")\n",
    "print(\"\\nTop 10 most duplicated headlines:\")\n",
    "print(duplicated_headlines.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "summary_counts = df_merged['summary'].value_counts()\n",
    "duplicated_summaries = summary_counts[summary_counts > 1]\n",
    "print(f\"Total number of duplicated summaries: {len(duplicated_summaries)}\")\n",
    "print(\"\\nTop 10 most duplicated summaries:\")\n",
    "print(duplicated_summaries.head(10))\n",
    "\n",
    "# Display the actual headlines/summaries with their dates\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated headline:\")\n",
    "if len(duplicated_headlines) > 0:\n",
    "    most_dup_headline = duplicated_headlines.index[0]\n",
    "    print(f\"Headline (appearing {duplicated_headlines.iloc[0]} times): {most_dup_headline}\")\n",
    "    print(\"\\nDates when this headline appeared:\")\n",
    "    for date in df_merged[df_merged['headline'] == most_dup_headline]['date']:\n",
    "        print(f\"- {date}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated summary:\")\n",
    "if len(duplicated_summaries) > 0:\n",
    "    most_dup_summary = duplicated_summaries.index[0]\n",
    "    print(f\"Summary (appearing {duplicated_summaries.iloc[0]} times): {most_dup_summary}\")\n",
    "    print(\"\\nDates when this summary appeared:\")\n",
    "    for date in df_merged[df_merged['summary'] == most_dup_summary]['date']:\n",
    "        print(f\"- {date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    open    high     low   close     volume  is_trading_day  \\\n",
      "0  2024-03-25  263.01  264.95  260.89  261.92   824685.0            True   \n",
      "11 2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "10 2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "8  2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "7  2024-03-27  267.00  269.72  257.56  260.20  1427218.0            True   \n",
      "\n",
      "                                             headline  \\\n",
      "0                                                       \n",
      "11  KeyBanc Keeps Their Buy Rating on Spotify Tech...   \n",
      "10  Taylor Swift Had A Monster Year, As Did The Mu...   \n",
      "8   Another List Throws Netflix (NASDAQ:NFLX) Out,...   \n",
      "7   Spotify initiated with bullish view at HSBC, h...   \n",
      "\n",
      "                                              summary  \n",
      "0                                                      \n",
      "11  Looking for stock market analysis and research...  \n",
      "10  Looking for stock market analysis and research...  \n",
      "8   Looking for stock market analysis and research...  \n",
      "7   Looking for stock market analysis and research...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1344 entries, 0 to 1443\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            1344 non-null   datetime64[ns]\n",
      " 1   open            1344 non-null   float64       \n",
      " 2   high            1344 non-null   float64       \n",
      " 3   low             1344 non-null   float64       \n",
      " 4   close           1344 non-null   float64       \n",
      " 5   volume          1344 non-null   float64       \n",
      " 6   is_trading_day  1344 non-null   bool          \n",
      " 7   headline        1344 non-null   object        \n",
      " 8   summary         1344 non-null   object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 95.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#lets delete the duplicates headlines and summaries from the dataframe\n",
    "df_merged = df_merged.drop_duplicates(subset=['headline', 'summary'])\n",
    "\n",
    "print(df_merged.head())\n",
    "print(df_merged.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe size: 1344 rows\n",
      "After removing specific summaries: 910 rows (removed 434 rows)\n",
      "After removing specific headlines: 910 rows (removed 0 rows)\n",
      "After removing duplicate headlines: 896 rows (removed 14 rows)\n",
      "After removing duplicate summaries: 879 rows (removed 17 rows)\n",
      "\n",
      "Total rows removed: 465\n",
      "Final dataframe size: 879 rows\n",
      "\n",
      "Remaining duplicated headlines: 0\n",
      "Remaining duplicated summaries: 0\n"
     ]
    }
   ],
   "source": [
    "# First, identify the specific summaries and headlines to completely remove\n",
    "summaries_to_remove = [\n",
    "    \"Looking for stock market analysis and research with proves results? Zacks.com offers in-depth financial research with over 30years of proven results.\",\n",
    "    \"\"  # Empty or whitespace summary\n",
    "]\n",
    "\n",
    "headlines_to_remove = [\"\"]  # Empty or whitespace headline\n",
    "\n",
    "# Get the original dataframe size\n",
    "original_size = len(df_merged)\n",
    "print(f\"Original dataframe size: {original_size} rows\")\n",
    "\n",
    "# Step 1: Completely remove rows with the specified summaries\n",
    "df_cleaned = df_merged[~df_merged['summary'].isin(summaries_to_remove)]\n",
    "step1_size = len(df_cleaned)\n",
    "print(f\"After removing specific summaries: {step1_size} rows (removed {original_size - step1_size} rows)\")\n",
    "\n",
    "# Step 2: Completely remove rows with the specified headlines\n",
    "df_cleaned = df_cleaned[~df_cleaned['headline'].isin(headlines_to_remove)]\n",
    "step2_size = len(df_cleaned)\n",
    "print(f\"After removing specific headlines: {step2_size} rows (removed {step1_size - step2_size} rows)\")\n",
    "\n",
    "# Step 3: For other duplicated headlines, keep only the first occurrence\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['headline'], keep='first')\n",
    "step3_size = len(df_cleaned)\n",
    "print(f\"After removing duplicate headlines: {step3_size} rows (removed {step2_size - step3_size} rows)\")\n",
    "\n",
    "# Step 4: For other duplicated summaries, keep only the first occurrence\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['summary'], keep='first')\n",
    "final_size = len(df_cleaned)\n",
    "print(f\"After removing duplicate summaries: {final_size} rows (removed {step3_size - final_size} rows)\")\n",
    "\n",
    "print(f\"\\nTotal rows removed: {original_size - final_size}\")\n",
    "print(f\"Final dataframe size: {final_size} rows\")\n",
    "\n",
    "# Check if there are any remaining duplicates\n",
    "remaining_dup_headlines = df_cleaned['headline'].duplicated().sum()\n",
    "remaining_dup_summaries = df_cleaned['summary'].duplicated().sum()\n",
    "print(f\"\\nRemaining duplicated headlines: {remaining_dup_headlines}\")\n",
    "print(f\"Remaining duplicated summaries: {remaining_dup_summaries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicated headlines: 0\n",
      "\n",
      "Top 10 most duplicated headlines:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Total number of duplicated summaries: 0\n",
      "\n",
      "Top 10 most duplicated summaries:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated headline:\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Details of most duplicated summary:\n"
     ]
    }
   ],
   "source": [
    "#Lets check if there are still any duplicates in the dataframe\n",
    "\n",
    "# Count duplicated headlines and summaries\n",
    "headline_counts = df_cleaned['headline'].value_counts()\n",
    "duplicated_headlines = headline_counts[headline_counts > 1]\n",
    "print(f\"Total number of duplicated headlines: {len(duplicated_headlines)}\")\n",
    "print(\"\\nTop 10 most duplicated headlines:\")\n",
    "print(duplicated_headlines.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "summary_counts = df_cleaned['summary'].value_counts()\n",
    "duplicated_summaries = summary_counts[summary_counts > 1]\n",
    "print(f\"Total number of duplicated summaries: {len(duplicated_summaries)}\")\n",
    "print(\"\\nTop 10 most duplicated summaries:\")\n",
    "print(duplicated_summaries.head(10))\n",
    "\n",
    "# Display the actual headlines/summaries with their dates\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated headline:\")\n",
    "if len(duplicated_headlines) > 0:\n",
    "    most_dup_headline = duplicated_headlines.index[0]\n",
    "    print(f\"Headline (appearing {duplicated_headlines.iloc[0]} times): {most_dup_headline}\")\n",
    "    print(\"\\nDates when this headline appeared:\")\n",
    "    for date in df_cleaned[df_cleaned['headline'] == most_dup_headline]['date']:\n",
    "        print(f\"- {date}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Details of most duplicated summary:\")\n",
    "if len(duplicated_summaries) > 0:\n",
    "    most_dup_summary = duplicated_summaries.index[0]\n",
    "    print(f\"Summary (appearing {duplicated_summaries.iloc[0]} times): {most_dup_summary}\")\n",
    "    print(\"\\nDates when this summary appeared:\")\n",
    "    for date in df_cleaned[df_cleaned['summary'] == most_dup_summary]['date']:\n",
    "        print(f\"- {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 879 entries, 3 to 1443\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            879 non-null    datetime64[ns]\n",
      " 1   open            879 non-null    float64       \n",
      " 2   high            879 non-null    float64       \n",
      " 3   low             879 non-null    float64       \n",
      " 4   close           879 non-null    float64       \n",
      " 5   volume          879 non-null    float64       \n",
      " 6   is_trading_day  879 non-null    bool          \n",
      " 7   headline        879 non-null    object        \n",
      " 8   summary         879 non-null    object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 62.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned Data\n",
    "\n",
    "Finally we have a cleaned dataset of all the news about Spotify and also stock data. \n",
    "\n",
    "### Two things to keep in mind:\n",
    "\n",
    "1. We used forward fill for the stock data to populate all dates that were not trading days.\n",
    "2. We have duplicate dates because there are days with many news articles and then again days without any news!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged dataframe\n",
    "df_cleaned.to_csv(\"/Users/armandocriscuolo/c2025/data_science_project_2025/code/Data-Science-Project/Data-Question-2-B/news_analysis/spotify_news_stock_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spotify stock and news sentiment analysis...\n",
      "Preprocessed data shape: (879, 13)\n",
      "Applying sentiment analysis to headlines and summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/armandocriscuolo/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating data to daily level...\n",
      "Creating lagged features up to 5 days...\n",
      "Analyzing correlations between sentiment and stock metrics...\n",
      "\n",
      "Key correlation findings:\n",
      "  - Sentiment vs daily_return: r=-0.0439 (p=0.4912, not significant)\n",
      "  - Sentiment vs next_day_return: r=0.1268 (p=0.0464, significant)\n",
      "  - Sentiment vs volatility_20d: r=-0.0269 (p=0.6772, not significant)\n",
      "Analyzing lagged effects of sentiment on returns...\n",
      "Correlation with next day's return: 0.1268\n",
      "Building prediction model for stock returns...\n",
      "Model evaluation - Mean MSE: 7.1883, Mean R²: -0.4544\n",
      "Creating visualizations...\n",
      "\n",
      "Analysis completed! Key findings:\n",
      "1. Strongest lagged effect: sentiment_lag_1 (r=0.1268)\n",
      "2. Most important predictive features: sentiment_lag_1, volume_change, news_count\n",
      "3. Model prediction performance: R²=-0.4544\n"
     ]
    }
   ],
   "source": [
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Download NLTK resources (run this once)\n",
    "try:\n",
    "    nltk.data.find('vader_lexicon')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the Spotify stock and news data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Raw dataframe with stock and news data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed dataframe with additional financial metrics\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure date column is datetime if needed\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Calculate daily returns (percentage change in closing price)\n",
    "    df['daily_return'] = df['close'].pct_change() * 100\n",
    "    \n",
    "    # Calculate volatility (20-day rolling standard deviation of returns)\n",
    "    df['volatility_20d'] = df['daily_return'].rolling(window=20).std()\n",
    "    \n",
    "    # Calculate abnormal returns (daily return - average market return)\n",
    "    # Note: In a real analysis, you would use market index returns\n",
    "    df['abnormal_return'] = df['daily_return'] - df['daily_return'].mean()\n",
    "    \n",
    "    # Calculate volume change\n",
    "    df['volume_change'] = df['volume'].pct_change() * 100\n",
    "    \n",
    "    # Log transform volume (often helps with analysis)\n",
    "    df['log_volume'] = np.log(df['volume'].replace(0, 1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_sentiment(df):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to news headlines and summaries\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with headline and summary columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Dataframe with added sentiment scores\n",
    "    \"\"\"\n",
    "    # Initialize VADER sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Fill NaN values with empty strings\n",
    "    df['headline'] = df['headline'].fillna('')\n",
    "    df['summary'] = df['summary'].fillna('')\n",
    "    \n",
    "    # Define a function to get sentiment scores\n",
    "    def get_sentiment(text):\n",
    "        if pd.isna(text) or text == '':\n",
    "            return 0\n",
    "        return sia.polarity_scores(str(text))['compound']\n",
    "    \n",
    "    # Apply sentiment analysis\n",
    "    print(\"Applying sentiment analysis to headlines and summaries...\")\n",
    "    df['headline_sentiment'] = df['headline'].apply(get_sentiment)\n",
    "    df['summary_sentiment'] = df['summary'].apply(get_sentiment)\n",
    "    \n",
    "    # Calculate weighted sentiment (headline has more impact)\n",
    "    df['combined_sentiment'] = 0.7 * df['headline_sentiment'] + 0.3 * df['summary_sentiment']\n",
    "    \n",
    "    # Create sentiment categories for analysis\n",
    "    df['sentiment_category'] = pd.cut(\n",
    "        df['combined_sentiment'],\n",
    "        bins=[-1.1, -0.2, 0.2, 1.1],\n",
    "        labels=['negative', 'neutral', 'positive']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def aggregate_daily_data(df):\n",
    "    \"\"\"\n",
    "    Aggregate multiple news items per day into single daily records\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with sentiment scores\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Dataframe with one row per day\n",
    "    \"\"\"\n",
    "    print(\"Aggregating data to daily level...\")\n",
    "    \n",
    "    # For news sentiment, we want the average per day\n",
    "    agg_functions = {\n",
    "        'headline_sentiment': 'mean',\n",
    "        'summary_sentiment': 'mean',\n",
    "        'combined_sentiment': 'mean',\n",
    "        # For stock data, we just take the first value (should be same for all rows on same day)\n",
    "        'open': 'first',\n",
    "        'high': 'first',\n",
    "        'low': 'first',\n",
    "        'close': 'first',\n",
    "        'volume': 'first',\n",
    "        'daily_return': 'first',\n",
    "        'volatility_20d': 'first',\n",
    "        'volume_change': 'first',\n",
    "        'abnormal_return': 'first',\n",
    "        'log_volume': 'first',\n",
    "        'is_trading_day': 'first'\n",
    "    }\n",
    "    \n",
    "    # Group by date and apply aggregation\n",
    "    daily_data = df.groupby(df.index).agg(agg_functions)\n",
    "    \n",
    "    # Count number of news articles per day\n",
    "    news_count = df.groupby(df.index).size()\n",
    "    daily_data['news_count'] = news_count\n",
    "    \n",
    "    return daily_data\n",
    "\n",
    "def create_lagged_features(df, max_lag=5):\n",
    "    \"\"\"\n",
    "    Create lagged features for time series analysis\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Daily dataframe\n",
    "    max_lag (int): Maximum number of days to lag\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Dataframe with lagged features\n",
    "    \"\"\"\n",
    "    print(f\"Creating lagged features up to {max_lag} days...\")\n",
    "    df_lagged = df.copy()\n",
    "    \n",
    "    # Create lagged sentiment features\n",
    "    for lag in range(1, max_lag+1):\n",
    "        df_lagged[f'sentiment_lag_{lag}'] = df_lagged['combined_sentiment'].shift(lag)\n",
    "    \n",
    "    # Create forward return (next day's return)\n",
    "    df_lagged['next_day_return'] = df_lagged['daily_return'].shift(-1)\n",
    "    \n",
    "    return df_lagged\n",
    "\n",
    "def analyze_correlation(df):\n",
    "    \"\"\"\n",
    "    Analyze correlation between sentiment and stock metrics\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Processed dataframe\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Correlation matrix and p-values matrix\n",
    "    \"\"\"\n",
    "    print(\"Analyzing correlations between sentiment and stock metrics...\")\n",
    "    \n",
    "    # Select relevant columns for correlation analysis\n",
    "    cols_for_corr = [\n",
    "        'headline_sentiment', 'summary_sentiment', 'combined_sentiment',\n",
    "        'daily_return', 'next_day_return', 'volatility_20d', 'volume_change', 'news_count'\n",
    "    ]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df[cols_for_corr].corr()\n",
    "    \n",
    "    # Calculate statistical significance\n",
    "    p_values = pd.DataFrame(index=corr_matrix.index, columns=corr_matrix.columns)\n",
    "    \n",
    "    for i in corr_matrix.index:\n",
    "        for j in corr_matrix.columns:\n",
    "            if i != j:  # Skip diagonal\n",
    "                # Create a temporary dataframe with just the two columns we're looking at\n",
    "                # and drop rows where either column has a NaN value\n",
    "                temp_df = df[[i, j]].dropna()\n",
    "                if len(temp_df) >= 2:  # Need at least 2 points for correlation\n",
    "                    corr, p = stats.pearsonr(temp_df[i], temp_df[j])\n",
    "                    p_values.loc[i, j] = p\n",
    "                else:\n",
    "                    p_values.loc[i, j] = np.nan\n",
    "    \n",
    "    return corr_matrix, p_values\n",
    "\n",
    "def analyze_lagged_effects(df, max_lag=5):\n",
    "    \"\"\"\n",
    "    Analyze the effect of lagged sentiment on stock returns\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with lagged features\n",
    "    max_lag (int): Maximum lag to analyze\n",
    "    \n",
    "    Returns:\n",
    "    pandas.Series: Correlations of lagged sentiment with returns\n",
    "    \"\"\"\n",
    "    print(\"Analyzing lagged effects of sentiment on returns...\")\n",
    "    \n",
    "    # Get correlation of each lagged sentiment with returns\n",
    "    lagged_columns = [f'sentiment_lag_{i}' for i in range(1, max_lag+1)]\n",
    "    lagged_corr = df[['daily_return'] + lagged_columns].corr().loc['daily_return', lagged_columns]\n",
    "    \n",
    "    # Get correlation with next day's return\n",
    "    next_day_corr = df[['combined_sentiment', 'next_day_return']].corr().loc['combined_sentiment', 'next_day_return']\n",
    "    print(f\"Correlation with next day's return: {next_day_corr:.4f}\")\n",
    "    \n",
    "    return lagged_corr\n",
    "\n",
    "def build_prediction_model(df, features=None):\n",
    "    \"\"\"\n",
    "    Build a machine learning model to predict returns based on sentiment\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Dataframe with sentiment and return data\n",
    "    features (list): List of feature columns to use\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Feature importances, MSE scores, and R² scores\n",
    "    \"\"\"\n",
    "    print(\"Building prediction model for stock returns...\")\n",
    "    \n",
    "    if features is None:\n",
    "        features = [\n",
    "            'combined_sentiment', 'news_count', 'volatility_20d',\n",
    "            'volume_change', 'sentiment_lag_1', 'sentiment_lag_2'\n",
    "        ]\n",
    "    \n",
    "    # Filter rows with complete data\n",
    "    model_data = df.dropna(subset=features + ['next_day_return'])\n",
    "    model_data = model_data[model_data['is_trading_day']]  # Only use trading days\n",
    "    \n",
    "    X = model_data[features]\n",
    "    y = model_data['next_day_return']\n",
    "    \n",
    "    # Use time series split for validation (important for time series data)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "    r2_scores = []\n",
    "    feature_importances = None\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train random forest model\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = rf.predict(X_test)\n",
    "        mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        \n",
    "        # Store feature importance\n",
    "        if feature_importances is None:\n",
    "            feature_importances = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': rf.feature_importances_\n",
    "            })\n",
    "        else:\n",
    "            feature_importances['importance'] = (feature_importances['importance'] + rf.feature_importances_) / 2\n",
    "    \n",
    "    print(f\"Model evaluation - Mean MSE: {np.mean(mse_scores):.4f}, Mean R²: {np.mean(r2_scores):.4f}\")\n",
    "    return feature_importances.sort_values('importance', ascending=False), mse_scores, r2_scores\n",
    "\n",
    "def visualize_results(df, daily_data, corr_matrix, lagged_corr, feature_importances):\n",
    "    \"\"\"\n",
    "    Create visualizations for the sentiment-stock analysis\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Original dataframe with sentiment\n",
    "    daily_data (pandas.DataFrame): Aggregated daily data\n",
    "    corr_matrix (pandas.DataFrame): Correlation matrix\n",
    "    lagged_corr (pandas.Series): Lagged correlation results\n",
    "    feature_importances (pandas.DataFrame): Feature importances from model\n",
    "    \"\"\"\n",
    "    print(\"Creating visualizations...\")\n",
    "    \n",
    "    # Figure 1: Stock price and sentiment over time\n",
    "    plt.figure(figsize=(12, 20))\n",
    "    \n",
    "    # Plot 1: Stock price and sentiment\n",
    "    plt.subplot(4, 1, 1)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(daily_data.index, daily_data['close'], 'b-', label='Close Price')\n",
    "    ax1.set_ylabel('Stock Price ($)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(daily_data.index, daily_data['combined_sentiment'], 'r-', alpha=0.7, label='Sentiment')\n",
    "    ax2.set_ylabel('Sentiment Score', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    plt.title('Spotify Stock Price and News Sentiment Over Time')\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    # Plot 2: Scatter plot of sentiment vs. daily return\n",
    "    plt.subplot(4, 1, 2)\n",
    "    trading_days = daily_data[daily_data['is_trading_day']]\n",
    "    sns.scatterplot(\n",
    "        x='combined_sentiment', \n",
    "        y='daily_return', \n",
    "        data=trading_days, \n",
    "        hue='news_count',\n",
    "        palette='viridis',\n",
    "        size='volume_change',\n",
    "        sizes=(20, 200),\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Add regression line\n",
    "    x = trading_days['combined_sentiment']\n",
    "    y = trading_days['daily_return']\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(x, p(x), \"r--\", alpha=0.7)\n",
    "    \n",
    "    plt.title('News Sentiment vs. Daily Return')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Daily Return (%)')\n",
    "    \n",
    "    # Plot 3: Correlation heatmap\n",
    "    plt.subplot(4, 1, 3)\n",
    "    sns.heatmap(\n",
    "        corr_matrix[['daily_return', 'next_day_return', 'volatility_20d', 'volume_change']]\n",
    "        .loc[['headline_sentiment', 'summary_sentiment', 'combined_sentiment', 'news_count']],\n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        vmin=-0.5, \n",
    "        vmax=0.5\n",
    "    )\n",
    "    plt.title('Correlation Between Sentiment and Stock Metrics')\n",
    "    \n",
    "    # Plot 4: Lagged effects analysis\n",
    "    plt.subplot(4, 1, 4)\n",
    "    lagged_corr.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Effect of Lagged Sentiment on Daily Returns')\n",
    "    plt.xlabel('Lag (Days)')\n",
    "    plt.ylabel('Correlation with Returns')\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('spotify_sentiment_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Figure 2: Additional insights\n",
    "    plt.figure(figsize=(12, 15))\n",
    "    \n",
    "    # Plot 1: Returns by sentiment category\n",
    "    plt.subplot(3, 1, 1)\n",
    "    \n",
    "    # Recalculate sentiment categories for the daily data\n",
    "    trading_days['sentiment_category'] = pd.cut(\n",
    "        trading_days['combined_sentiment'],\n",
    "        bins=[-1.1, -0.2, 0.2, 1.1],\n",
    "        labels=['negative', 'neutral', 'positive']\n",
    "    )\n",
    "    \n",
    "    sns.boxplot(x='sentiment_category', y='daily_return', data=trading_days)\n",
    "    plt.title('Daily Returns by Sentiment Category')\n",
    "    plt.xlabel('Sentiment Category')\n",
    "    plt.ylabel('Daily Return (%)')\n",
    "    \n",
    "    # Perform ANOVA to check if differences are significant\n",
    "    sentiment_groups = [\n",
    "        trading_days[trading_days['sentiment_category'] == 'negative']['daily_return'].dropna(),\n",
    "        trading_days[trading_days['sentiment_category'] == 'neutral']['daily_return'].dropna(),\n",
    "        trading_days[trading_days['sentiment_category'] == 'positive']['daily_return'].dropna()\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        f_stat, p_val = stats.f_oneway(*sentiment_groups)\n",
    "        plt.annotate(f\"ANOVA: F={f_stat:.2f}, p={p_val:.4f}\", \n",
    "                    xy=(0.5, 0.9), xycoords='axes fraction', \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Plot 2: Feature importance from prediction model\n",
    "    plt.subplot(3, 1, 2)\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importances, palette='viridis')\n",
    "    plt.title('Feature Importance for Return Prediction')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "    # Plot 3: News count distribution and impact\n",
    "    plt.subplot(3, 1, 3)\n",
    "    sns.scatterplot(x='news_count', y='volatility_20d', \n",
    "                   size='combined_sentiment', hue='daily_return',\n",
    "                   data=trading_days, palette='RdYlGn', sizes=(20, 200))\n",
    "    plt.title('News Volume vs. Volatility')\n",
    "    plt.xlabel('Number of News Articles')\n",
    "    plt.ylabel('20-Day Volatility')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('spotify_sentiment_additional_insights.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "def run_sentiment_analysis(df_cleaned):\n",
    "    \"\"\"\n",
    "    Main function to run the complete sentiment analysis pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    df_cleaned (pandas.DataFrame): The cleaned Spotify stock and news data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Results of the analysis\n",
    "    \"\"\"\n",
    "    print(\"Starting Spotify stock and news sentiment analysis...\")\n",
    "    \n",
    "    # Step 1: Preprocess data\n",
    "    df_processed = preprocess_data(df_cleaned)\n",
    "    print(f\"Preprocessed data shape: {df_processed.shape}\")\n",
    "    \n",
    "    # Step 2: Apply sentiment analysis\n",
    "    df_with_sentiment = analyze_sentiment(df_processed)\n",
    "    \n",
    "    # Step 3: Aggregate to daily level\n",
    "    daily_data = aggregate_daily_data(df_with_sentiment)\n",
    "    \n",
    "    # Step 4: Create lagged features\n",
    "    daily_data_with_lags = create_lagged_features(daily_data)\n",
    "    \n",
    "    # Step 5: Analyze correlation\n",
    "    corr_matrix, p_values = analyze_correlation(daily_data_with_lags)\n",
    "    \n",
    "    # Print key correlations\n",
    "    print(\"\\nKey correlation findings:\")\n",
    "    key_correlations = corr_matrix.loc['combined_sentiment', ['daily_return', 'next_day_return', 'volatility_20d']]\n",
    "    key_p_values = p_values.loc['combined_sentiment', ['daily_return', 'next_day_return', 'volatility_20d']]\n",
    "    for metric, corr in key_correlations.items():\n",
    "        p = key_p_values[metric]\n",
    "        significance = \"significant\" if p < 0.05 else \"not significant\"\n",
    "        print(f\"  - Sentiment vs {metric}: r={corr:.4f} (p={p:.4f}, {significance})\")\n",
    "    \n",
    "    # Step 6: Analyze lagged effects\n",
    "    lagged_corr = analyze_lagged_effects(daily_data_with_lags)\n",
    "    \n",
    "    # Step 7: Build prediction model\n",
    "    feature_importances, mse_scores, r2_scores = build_prediction_model(daily_data_with_lags)\n",
    "    \n",
    "    # Step 8: Visualize results\n",
    "    visualize_results(df_with_sentiment, daily_data, corr_matrix, lagged_corr, feature_importances)\n",
    "    \n",
    "    # Step 9: Prepare summary of findings\n",
    "    print(\"\\nAnalysis completed! Key findings:\")\n",
    "    \n",
    "    # Determine most influential sentiment lag\n",
    "    max_lag_idx = lagged_corr.abs().idxmax()\n",
    "    max_lag_corr = lagged_corr[max_lag_idx]\n",
    "    max_lag = int(max_lag_idx.split('_')[-1])\n",
    "    \n",
    "    print(f\"1. Strongest lagged effect: {max_lag_idx} (r={max_lag_corr:.4f})\")\n",
    "    print(f\"2. Most important predictive features: {', '.join(feature_importances['feature'].head(3).tolist())}\")\n",
    "    print(f\"3. Model prediction performance: R²={np.mean(r2_scores):.4f}\")\n",
    "    \n",
    "    # Return results for further analysis if needed\n",
    "    results = {\n",
    "        'df_with_sentiment': df_with_sentiment,\n",
    "        'daily_data': daily_data,\n",
    "        'daily_data_with_lags': daily_data_with_lags,\n",
    "        'corr_matrix': corr_matrix,\n",
    "        'p_values': p_values,\n",
    "        'lagged_corr': lagged_corr,\n",
    "        'feature_importances': feature_importances,\n",
    "        'mse_scores': mse_scores,\n",
    "        'r2_scores': r2_scores\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute the analysis\n",
    "results = run_sentiment_analysis(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "### The Question: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
